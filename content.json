{"meta":{"title":"skyitachi's blog","subtitle":null,"description":"一定要有输出","author":"skyitachi","url":"https://skyitachi.github.io","root":"/"},"pages":[],"posts":[{"title":"Lucene中的HNSW索引是如何写入的","slug":"Lucene中的hnsw索引是如何写入的","date":"2024-08-07T16:00:00.000Z","updated":"2024-08-07T16:00:00.000Z","comments":true,"path":"2024/08/08/Lucene中的hnsw索引是如何写入的/","permalink":"https://skyitachi.github.io/2024/08/08/Lucene%E4%B8%AD%E7%9A%84hnsw%E7%B4%A2%E5%BC%95%E6%98%AF%E5%A6%82%E4%BD%95%E5%86%99%E5%85%A5%E7%9A%84/","excerpt":"","text":"主要内容 梳理IndexWriter.addDocuments 和 IndexWriter.commit的调用栈 梳理向量索引在Lucene中是如何被存储的主要调用路径，包括如何原始向量如何保存到内存和文件，HNSW索引的构建以及如何持久化的 Lucene在处理Segment Merge的过程中是如何处理HNSW索引的 调用栈 调用栈对应的示例对应的Lucene版本: releases&#x2F;lucene&#x2F;9.9.0 12345678910111213141516171819202122232425262728293031323334353637383940414243Path docPath = Paths.get(&quot;data/lucene_knn_demo&quot;);if (Files.exists(docPath)) &#123; String absPath = docPath.toAbsolutePath().toString(); File directory = new File(absPath); FileUtils.deleteDirectory(directory);&#125;Directory directory = FSDirectory.open(docPath);IndexWriterConfig config = new IndexWriterConfig();FileOutputStream output = new FileOutputStream(&quot;lucene_knn.log&quot;);InfoStream customInfoStream = new PrintStreamInfoStream(new PrintStream(output));System.out.println(&quot;IW Component is enabled: &quot; + customInfoStream.isEnabled(&quot;IW&quot;));config.setInfoStream(customInfoStream);config.setUseCompoundFile(false);IndexWriter indexWriter = new IndexWriter(directory, config);int count = 10000;int dim = 128;List&lt;Document&gt; docs = new ArrayList&lt;&gt;();for (int i = 0; i &lt; count; i++) &#123; Document doc = new Document(); doc.add(new KeywordField(&quot;id&quot;, Integer.toString(i), Field.Store.YES)); doc.add(new KnnFloatVectorField(&quot;fvecs&quot;, generateFVector(dim))); docs.add(doc);&#125;long start = System.currentTimeMillis();indexWriter.addDocuments(docs);indexWriter.commit();System.out.printf(&quot;%d vectors consumes: %d ms\\n&quot;, count, System.currentTimeMillis() - start);start = System.currentTimeMillis();// indexWriter.forceMerge(1);// System.out.printf(&quot;forceMerge consumes: %d ms\\n&quot;, System.currentTimeMillis() - start);indexWriter.close(); Lucene如何在内存中存储向量的向量相关的分为两个部分，一个是原始向量，一个是向量索引 内存中向量的存储原始向量从调用链路上看，indexChain.indexVectorValue 涉及到了向量相关的存储，根据向量类型的(Float32或者Byte), 本文例子中的使用Float32数组，最终会调用Lucene99FlatVectorsWriter.FieldWriter.addValue, 下面详细分析一下源码 12345678910111213141516171819public void addValue(int docID, T vectorValue) throws IOException &#123; if (docID == lastDocID) &#123; throw new IllegalArgumentException( &quot;VectorValuesField \\&quot;&quot; + fieldInfo.name + &quot;\\&quot; appears more than once in this document (only one value is allowed per field)&quot;); &#125; assert docID &gt; lastDocID; // 首先从IndexableField里的VectorValues复制一份原始向量 T copy = copyValue(vectorValue); docsWithField.add(docID); // vectors 是List&lt;T&gt; 对象，说明原始向量在内存中直接存入到一个List中 vectors.add(copy); lastDocID = docID; if (indexingDelegate != null) &#123; // 这里就是索引相关的计算 indexingDelegate.addValue(docID, copy); &#125;&#125; 索引内存中HNSW索引的构建的入口是indexingDelegate.addValue, 在本文中indexingDelegate就是Lucene99HnswVectorsWriter.FieldWriter, 下面简要分析一下其源码 12345678910111213141516public void addValue(int docID, T vectorValue) throws IOException &#123; if (docID == lastDocID) &#123; throw new IllegalArgumentException( &quot;VectorValuesField \\&quot;&quot; + fieldInfo.name + &quot;\\&quot; appears more than once in this document (only one value is allowed per field)&quot;); &#125; assert docID &gt; lastDocID; // 原始向量又被存储了一次 vectors.add(vectorValue); docsWithField.add(docID); // 构建索引 hnswGraphBuilder.addGraphNode(node); node++; lastDocID = docID;&#125; HNSW索引是如何在内存中表示的，Lucene使用了OnHeapHnswGraph表示了索引相关信息的存储（主要是节点之间的连接信息）. 12345678910111213141516171819202122232425// 省略了其他一些字段public final class OnHeapHnswGraph extends HnswGraph implements Accountable &#123; // ... private final AtomicReference&lt;EntryNode&gt; entryNode; // 表示了HNSW中整个索引的entryPoint // the internal graph representation where the first dimension is node id and second dimension is // level // e.g. graph[1][2] is all the neighbours of node 1 at level 2 // 索引中节点的邻接信息 private NeighborArray[][] graph; OnHeapHnswGraph(int M, int numNodes) &#123; this.entryNode = new AtomicReference&lt;&gt;(new EntryNode(-1, 1)); // Neighbours&#x27; size on upper levels (nsize) and level 0 (nsize0) // We allocate extra space for neighbours, but then prune them to keep allowed maximum this.nsize = M + 1; this.nsize0 = (M * 2 + 1); noGrowth = numNodes != -1; if (noGrowth == false) &#123; numNodes = INIT_SIZE; &#125; this.graph = new NeighborArray[numNodes][]; &#125;&#125; 文件中向量的存储向量相关的索引文件涉及四种 *.vec 存储原始向量 *.vemf 原始向量的meta文件 *.vex 向量索引文件, Lucene中目前用的HNSW索引 *.vem 向量索引文件的meta文件 向量相关的文件写入（flush）是由Lucene99HnswVectorsWriter.flush实现的 12345678910111213public void flush(int maxDoc, Sorter.DocMap sortMap) throws IOException &#123; // 写入原始向量 flatVectorWriter.flush(maxDoc, sortMap); for (FieldWriter&lt;?&gt; field : fields) &#123; if (sortMap == null) &#123; writeField(field); &#125; else &#123; writeSortingField(field, sortMap); &#125; &#125;&#125; 原始向量的写入 Lucene99FlatVectorsWriter.flush 12345678910111213141516171819202122232425262728293031323334353637383940public void flush(int maxDoc, Sorter.DocMap sortMap) throws IOException &#123; for (FieldWriter&lt;?&gt; field : fields) &#123; if (sortMap == null) &#123; writeField(field, maxDoc); &#125; else &#123; writeSortingField(field, maxDoc, sortMap); &#125; &#125;&#125;private void writeField(FieldWriter&lt;?&gt; fieldData, int maxDoc) throws IOException &#123; // write vector values long vectorDataOffset = vectorData.alignFilePointer(Float.BYTES); switch (fieldData.fieldInfo.getVectorEncoding()) &#123; case BYTE: writeByteVectors(fieldData); break; case FLOAT32: // 写入到原始向量文件里 writeFloat32Vectors(fieldData); break; &#125; long vectorDataLength = vectorData.getFilePointer() - vectorDataOffset; writeMeta( fieldData.fieldInfo, maxDoc, vectorDataOffset, vectorDataLength, fieldData.docsWithField);&#125;private void writeFloat32Vectors(FieldWriter&lt;?&gt; fieldData) throws IOException &#123; final ByteBuffer buffer = ByteBuffer.allocate(fieldData.dim * Float.BYTES).order(ByteOrder.LITTLE_ENDIAN); // 将之前存在内存中的vector的list写入到vectorData对应的文件里 for (Object v : fieldData.vectors) &#123; buffer.asFloatBuffer().put((float[]) v); vectorData.writeBytes(buffer.array(), buffer.array().length); &#125;&#125; 向量索引相关的写入 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136// 每个字段对应向量索引的写入private void writeField(FieldWriter&lt;?&gt; fieldData) throws IOException &#123; // write graph long vectorIndexOffset = vectorIndex.getFilePointer(); OnHeapHnswGraph graph = fieldData.getGraph(); int[][] graphLevelNodeOffsets = writeGraph(graph); // 向量索引的长度 long vectorIndexLength = vectorIndex.getFilePointer() - vectorIndexOffset; // graphLevelNodeOffsets 经过编码后会写入到vectorIndexOutput中，而不是meta文件中 writeMeta( fieldData.fieldInfo, vectorIndexOffset, vectorIndexLength, fieldData.docsWithField.cardinality(), graph, graphLevelNodeOffsets);&#125;// hnsw节点连接信息的写入private int[][] writeGraph(OnHeapHnswGraph graph) throws IOException &#123; if (graph == null) return new int[0][0]; // write vectors&#x27; neighbours on each level into the vectorIndex file int countOnLevel0 = graph.size(); int[][] offsets = new int[graph.numLevels()][]; for (int level = 0; level &lt; graph.numLevels(); level++) &#123; // graph.getNodesOnLevel获取到每个level出现的node数组 int[] sortedNodes = NodesIterator.getSortedNodes(graph.getNodesOnLevel(level)); offsets[level] = new int[sortedNodes.length]; int nodeOffsetId = 0; for (int node : sortedNodes) &#123; // 根据node大小获取该node在level层级的节点连接信息 NeighborArray neighbors = graph.getNeighbors(level, node); int size = neighbors.size(); // Write size in VInt as the neighbors list is typically small long offsetStart = vectorIndex.getFilePointer(); // 写入这个节点对应的连接数 vectorIndex.writeVInt(size); // Destructively modify; it&#x27;s ok we are discarding it after this int[] nnodes = neighbors.node(); // 根据node大小做升序排序，方便后面做差值压缩编码 Arrays.sort(nnodes, 0, size); // Now that we have sorted, do delta encoding to minimize the required bits to store the // information for (int i = size - 1; i &gt; 0; --i) &#123; assert nnodes[i] &lt; countOnLevel0 : &quot;node too large: &quot; + nnodes[i] + &quot;&gt;=&quot; + countOnLevel0; nnodes[i] -= nnodes[i - 1]; &#125; // 顺序写入经过差值压缩后的编码，nnodes[0]是原值 for (int i = 0; i &lt; size; i++) &#123; vectorIndex.writeVInt(nnodes[i]); &#125; // 将这个节点的连接信息的长度记录到offset表中 offsets[level][nodeOffsetId++] = Math.toIntExact(vectorIndex.getFilePointer() - offsetStart); &#125; &#125; return offsets;&#125;// 图索引meta信息的写入 private void writeMeta( FieldInfo field, long vectorIndexOffset, long vectorIndexLength, int count, HnswGraph graph, int[][] graphLevelNodeOffsets) throws IOException &#123; meta.writeInt(field.number); // 向量的编码FLOAT or BYTES meta.writeInt(field.getVectorEncoding().ordinal()); // 向量距离的计算方式 meta.writeInt(field.getVectorSimilarityFunction().ordinal()); // 向量索引的开始位置 meta.writeVLong(vectorIndexOffset); // 向量索引的总长度 meta.writeVLong(vectorIndexLength); // 向量的维度 meta.writeVInt(field.getVectorDimension()); // 向量的数量 meta.writeInt(count); meta.writeVInt(M); // write graph nodes on each level if (graph == null) &#123; meta.writeVInt(0); &#125; else &#123; // HNSW索引的层级 meta.writeVInt(graph.numLevels()); long valueCount = 0; for (int level = 0; level &lt; graph.numLevels(); level++) &#123; // 获取每个层级的节点 NodesIterator nodesOnLevel = graph.getNodesOnLevel(level); valueCount += nodesOnLevel.size(); if (level &gt; 0) &#123; // 非level0 的才需要写入meta int[] nol = new int[nodesOnLevel.size()]; int numberConsumed = nodesOnLevel.consume(nol); // 方便后面差值编码 Arrays.sort(nol); assert numberConsumed == nodesOnLevel.size(); // 写入该层级的节点数 meta.writeVInt(nol.length); // number of nodes on a level for (int i = nodesOnLevel.size() - 1; i &gt; 0; --i) &#123; nol[i] -= nol[i - 1]; &#125; for (int n : nol) &#123; assert n &gt;= 0 : &quot;delta encoding for nodes failed; expected nodes to be sorted&quot;; // 写入该层级的节点压缩编码 meta.writeVInt(n); &#125; &#125; else &#123; assert nodesOnLevel.size() == count : &quot;Level 0 expects to have all nodes&quot;; &#125; &#125; long start = vectorIndex.getFilePointer(); // 记录vectorIndex当前的文件位置（已经写完graph的连接信息的位置） meta.writeLong(start); meta.writeVInt(DIRECT_MONOTONIC_BLOCK_SHIFT); // graphLevelNodeOffsets的压缩编码, valueCount是说有level的总连接点的数量 // graphLevelNodeOffsets写入的也是VectorIndex final DirectMonotonicWriter memoryOffsetsWriter = DirectMonotonicWriter.getInstance( meta, vectorIndex, valueCount, DIRECT_MONOTONIC_BLOCK_SHIFT); long cumulativeOffsetSum = 0; for (int[] levelOffsets : graphLevelNodeOffsets) &#123; for (int v : levelOffsets) &#123; memoryOffsetsWriter.add(cumulativeOffsetSum); cumulativeOffsetSum += v; &#125; &#125; memoryOffsetsWriter.finish(); meta.writeLong(vectorIndex.getFilePointer() - start); &#125; &#125; 小结 Lucene中对整个向量相关的存储整体是分成两部分（原始向量和HNSW向量索引）. 在存储向量的索引过程中整体分成两个步骤 第一步存储各个层级节点的连接信息（graph），通过对连接节点的排序，存储差值对连接信息进行了压缩，在写入节点信息的过程过程中，会生成offset表，表示了每个层级中节点对应的连接信息的offset 第二步存储第一步生成的offset表，利用了DirectMonotonicWriter 进行了压缩处理 整体可以看到，Lucene并没有存储索引的原始信息，而是经过一系列精巧的压缩处理，减少了磁盘占用，后面的文章中会深入探讨这种压缩处理的利弊 Lucene在Merge的过程中如何处理HNSW索引的上文介绍的内容都是Lucene在生成一个Segment的过程中对向量及其索引的计算和存储，在实际的系统中，比如Elasticsearch一般都会有多个Lucene Segment的生成，Lucene本身也会对多个小Segment进行Merge, 我们需要知道在merge的过程中Lucene是如何处理向量相关的索引的 本文使用IndexWriter.forceMerge(1)触发merge操作，得到下文的调用栈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167 public void mergeOneField(FieldInfo fieldInfo, MergeState mergeState) throws IOException &#123; CloseableRandomVectorScorerSupplier scorerSupplier = flatVectorWriter.mergeOneFieldToIndex(fieldInfo, mergeState); boolean success = false; try &#123; long vectorIndexOffset = vectorIndex.getFilePointer(); // build the graph using the temporary vector data // we use Lucene99HnswVectorsReader.DenseOffHeapVectorValues for the graph construction // doesn&#x27;t need to know docIds // TODO: separate random access vector values from DocIdSetIterator? OnHeapHnswGraph graph = null; int[][] vectorIndexNodeOffsets = null; if (scorerSupplier.totalVectorCount() &gt; 0) &#123; // build graph // Merger是IncrementalHnswGraphMerger HnswGraphMerger merger = createGraphMerger(fieldInfo, scorerSupplier); for (int i = 0; i &lt; mergeState.liveDocs.length; i++) &#123; // 这里会选出节点数最大的graph，作为merge过程中初始的graph以减少图的构建 merger.addReader( mergeState.knnVectorsReaders[i], mergeState.docMaps[i], mergeState.liveDocs[i]); &#125; final DocIdSetIterator mergedVectorIterator; switch (fieldInfo.getVectorEncoding()) &#123; case BYTE: mergedVectorIterator = KnnVectorsWriter.MergedVectorValues.mergeByteVectorValues(fieldInfo, mergeState); break; case FLOAT32: // 获取每个Segment对应的原始向量，使用 OffHeapFloatVectorValues.load 获取每个原始向量 mergedVectorIterator = KnnVectorsWriter.MergedVectorValues.mergeFloatVectorValues(fieldInfo, mergeState); break; default: throw new IllegalStateException( &quot;Unsupported vector encoding: &quot; + fieldInfo.getVectorEncoding()); &#125; // 多个Segment中的HNSW graph合并成一个OnHeapHnswGraph graph = merger.merge( mergedVectorIterator, segmentWriteState.infoStream, scorerSupplier.totalVectorCount()); // 这里的写入和上文中的处理是一致的 vectorIndexNodeOffsets = writeGraph(graph); &#125; // 这里同上文也是一致的 long vectorIndexLength = vectorIndex.getFilePointer() - vectorIndexOffset; writeMeta( fieldInfo, vectorIndexOffset, vectorIndexLength, scorerSupplier.totalVectorCount(), graph, vectorIndexNodeOffsets); success = true; &#125; finally &#123; if (success) &#123; IOUtils.close(scorerSupplier); &#125; else &#123; IOUtils.closeWhileHandlingException(scorerSupplier); &#125; &#125; &#125;// 选出最大的graph 作为初始graphpublic IncrementalHnswGraphMerger addReader( KnnVectorsReader reader, MergeState.DocMap docMap, Bits liveDocs) throws IOException &#123; KnnVectorsReader currKnnVectorsReader = reader; if (reader instanceof PerFieldKnnVectorsFormat.FieldsReader) &#123; currKnnVectorsReader = ((PerFieldKnnVectorsFormat.FieldsReader) reader).getFieldReader(fieldInfo.name); &#125; if (!(currKnnVectorsReader instanceof HnswGraphProvider) || !noDeletes(liveDocs)) &#123; return this; &#125; int candidateVectorCount = 0; switch (fieldInfo.getVectorEncoding()) &#123; case BYTE: ByteVectorValues byteVectorValues = currKnnVectorsReader.getByteVectorValues(fieldInfo.name); if (byteVectorValues == null) &#123; return this; &#125; candidateVectorCount = byteVectorValues.size(); break; case FLOAT32: FloatVectorValues vectorValues = currKnnVectorsReader.getFloatVectorValues(fieldInfo.name); if (vectorValues == null) &#123; return this; &#125; candidateVectorCount = vectorValues.size(); break; default: throw new IllegalStateException( &quot;Unexpected vector encoding: &quot; + fieldInfo.getVectorEncoding()); &#125; // 选取最大节点数（向量数）的graph作为初始化的graph，这里使用initReader代替，后续使用initReader读取graph的时候就以这个graph作为初始graph if (candidateVectorCount &gt; initGraphSize) &#123; initReader = currKnnVectorsReader; initDocMap = docMap; initGraphSize = candidateVectorCount; &#125; return this; &#125;// 处理merge的入口 @Override public OnHeapHnswGraph merge( DocIdSetIterator mergedVectorIterator, InfoStream infoStream, int maxOrd) throws IOException &#123; // 创建 InitializedHnswGraphBuilder 对象, 同时会将最大的初始化graph load进内存变成`OnHeapHnswGraph` HnswBuilder builder = createBuilder(mergedVectorIterator, maxOrd); builder.setInfoStream(infoStream); return builder.build(maxOrd); &#125;// 初始化graph的 init过程 public static OnHeapHnswGraph initGraph( int M, HnswGraph initializerGraph, int[] newOrdMap, int totalNumberOfVectors) throws IOException &#123; // 用新的总向量数量和代替之前的节点数 OnHeapHnswGraph hnsw = new OnHeapHnswGraph(M, totalNumberOfVectors); for (int level = initializerGraph.numLevels() - 1; level &gt;= 0; level--) &#123; HnswGraph.NodesIterator it = initializerGraph.getNodesOnLevel(level); while (it.hasNext()) &#123; int oldOrd = it.nextInt(); int newOrd = newOrdMap[oldOrd]; hnsw.addNode(level, newOrd); hnsw.trySetNewEntryNode(newOrd, level); NeighborArray newNeighbors = hnsw.getNeighbors(level, newOrd); initializerGraph.seek(level, oldOrd); // 直接复制之前的连接信息 for (int oldNeighbor = initializerGraph.nextNeighbor(); oldNeighbor != NO_MORE_DOCS; oldNeighbor = initializerGraph.nextNeighbor()) &#123; int newNeighbor = newOrdMap[oldNeighbor]; // we will compute these scores later when we need to pop out the non-diverse nodes newNeighbors.addOutOfOrder(newNeighbor, Float.NaN); &#125; &#125; &#125; return hnsw; &#125;// merge过程中处理Vector相关的代码 protected void addVectors(int minOrd, int maxOrd) throws IOException &#123; long start = System.nanoTime(), t = start; if (infoStream.isEnabled(HNSW_COMPONENT)) &#123; infoStream.message(HNSW_COMPONENT, &quot;addVectors [&quot; + minOrd + &quot; &quot; + maxOrd + &quot;)&quot;); &#125; for (int node = minOrd; node &lt; maxOrd; node++) &#123; // 添加新的node到graph中 addGraphNode(node); if ((node % 10000 == 0) &amp;&amp; infoStream.isEnabled(HNSW_COMPONENT)) &#123; t = printGraphBuildStatus(node, start, t); &#125; &#125; &#125;// 实际的addGraphNode 调用，`InitializedHnswGraphBuilder.addGraphNode` public void addGraphNode(int node) throws IOException &#123; // merge的时候会选出节点数最多的索引作为初始的graph, 初始graph的所有信息都是构建好的，所以就不要添加了 if (initializedNodes.get(node)) &#123; return; &#125; // 向当前的graph中添加节点，执行标准hnsw构建流程 super.addGraphNode(node); &#125; 小结 merge过程中，核心点是选出一个最大的graph作为初始化graph，然后将其他segment的hsnw graph 写入到这个初始化graph中（省去了一些计算）,不过由于其他segment的graph的节点仍然需要重新计算, 导致merge的cpu成本也比较高 merge过程中，所有的向量都会被读取到OnHeapHnswGraph中，所以内存压力比较高 对于HNSW 索引来说，merge一个成本高昂的操作","categories":[],"tags":[{"name":"algorithm hnsw 向量数据库 Lucene Indexing","slug":"algorithm-hnsw-向量数据库-Lucene-Indexing","permalink":"https://skyitachi.github.io/tags/algorithm-hnsw-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-Lucene-Indexing/"}]},{"title":"Lucene中Term相关的倒排文件是如何生成的","slug":"Lucene中Term相关的倒排文件是如何生成的","date":"2024-08-03T16:00:00.000Z","updated":"2024-08-03T16:00:00.000Z","comments":true,"path":"2024/08/04/Lucene中Term相关的倒排文件是如何生成的/","permalink":"https://skyitachi.github.io/2024/08/04/Lucene%E4%B8%ADTerm%E7%9B%B8%E5%85%B3%E7%9A%84%E5%80%92%E6%8E%92%E6%96%87%E4%BB%B6%E6%98%AF%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90%E7%9A%84/","excerpt":"","text":"前言Lucene中Term代表了一个原子粒度的索引和搜索单位，ES中keyword和text类型经过分词之后都会以Term的形式被Lucene索引并落入磁盘持久化，与Term相关的索引文件有Term Dictionary（.tim），Term Index(.tip), 还有与之关联的倒排索引相关的文件(.doc, .pos, .pay)，本文重点关注在如何将内存中已经生成好的Term相关的数据结构落入磁盘的这一过程. 本文以Lucene tag releases/lucene/9.1.0 代码为准, commit: 5b522487ba8e0f1002b50a136817ca037aec9686, 只关注posting list中docId的写入，freq, position和payload 将会忽略 调用链路 相关源码分析主要分析Lucene90BlockTreeTermsWriter的写入过程, 核心函数是TermsWriter.write Lucene90BlockTreeTermsWriter.write -&gt; TermsWriter.write 12345678910111213141516171819202122232425262728// Lucene90BlockTreeTermsWriter.TermsWriter.write// TermsEnum 代表了这个Field关联的所有termspublic void write(BytesRef text, TermsEnum termsEnum, NormsProducer norms) throws IOException &#123; // 首先写入倒排索引相关的问题, postingWriter是Lucene90PostingWriter的实例, 包括了docId和skip List相关的数据 BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen, norms); if (state != null) &#123; assert state.docFreq != 0; assert fieldInfo.getIndexOptions() == IndexOptions.DOCS || state.totalTermFreq &gt;= state.docFreq : &quot;postingsWriter=&quot; + postingsWriter; pushTerm(text); PendingTerm term = new PendingTerm(text, state); pending.add(term); // if (DEBUG) System.out.println(&quot; add pending term = &quot; + text + &quot; pending.size()=&quot; + // pending.size()); sumDocFreq += state.docFreq; sumTotalTermFreq += state.totalTermFreq; numTerms++; if (firstPendingTerm == null) &#123; firstPendingTerm = term; &#125; lastPendingTerm = term; &#125;&#125; PushPostingsWriterBase.writeTerm 抽象了writeTerm的整体流程，Lucene90PostingWriter extends PushPostingsWriterBase 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 public final BlockTermState writeTerm( BytesRef term, TermsEnum termsEnum, FixedBitSet docsSeen, NormsProducer norms) throws IOException &#123; // 本文不涉及 NumericDocValues normValues; if (fieldInfo.hasNorms() == false) &#123; normValues = null; &#125; else &#123; normValues = norms.getNorms(fieldInfo); &#125; // 写入这个Term的一些初始化，包括lastDocId和skipList的初始化, 调用的实例方法是Lucene90PostingWriter.startTerm startTerm(normValues); // 从term中取出对应的posting list的iterator，本文主要关注posting中的docId postingsEnum = termsEnum.postings(postingsEnum, enumFlags); assert postingsEnum != null; int docFreq = 0; long totalTermFreq = 0; while (true) &#123; int docID = postingsEnum.nextDoc(); if (docID == PostingsEnum.NO_MORE_DOCS) &#123; break; &#125; docFreq++; docsSeen.set(docID); int freq; if (writeFreqs) &#123; freq = postingsEnum.freq(); totalTermFreq += freq; &#125; else &#123; freq = -1; &#125; // posting相关的核心函数, Lucene90PostingsWriter.startDoc startDoc(docID, freq); if (writePositions) &#123; for (int i = 0; i &lt; freq; i++) &#123; int pos = postingsEnum.nextPosition(); BytesRef payload = writePayloads ? postingsEnum.getPayload() : null; int startOffset; int endOffset; if (writeOffsets) &#123; startOffset = postingsEnum.startOffset(); endOffset = postingsEnum.endOffset(); &#125; else &#123; startOffset = -1; endOffset = -1; &#125; addPosition(pos, payload, startOffset, endOffset); &#125; &#125; finishDoc(); &#125; if (docFreq == 0) &#123; return null; &#125; else &#123; BlockTermState state = newTermState(); state.docFreq = docFreq; state.totalTermFreq = writeFreqs ? totalTermFreq : -1; finishTerm(state); return state; &#125;&#125; Lucene90PostingsWriter.startDoc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 public void startDoc(int docID, int termDocFreq) throws IOException &#123;// 为新的posting block创建(初始化)skipList if (lastBlockDocID != -1 &amp;&amp; docBufferUpto == 0) &#123; skipWriter.bufferSkip( lastBlockDocID, competitiveFreqNormAccumulator, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockPayloadByteUpto); competitiveFreqNormAccumulator.clear(); &#125;// posting 存储的docId 都是差值，且要求所有docId都是严格升序排列，隐含条件docId不会重复(相同Term在posting中只会出现一次) final int docDelta = docID - lastDocID; if (docID &lt; 0 || (docCount &gt; 0 &amp;&amp; docDelta &lt;= 0)) &#123; throw new CorruptIndexException( &quot;docs out of order (&quot; + docID + &quot; &lt;= &quot; + lastDocID + &quot; )&quot;, docOut); &#125; docDeltaBuffer[docBufferUpto] = docDelta; if (writeFreqs) &#123; freqBuffer[docBufferUpto] = termDocFreq; &#125;// block内docCount的计数 docBufferUpto++;// 整体的docCount 计数 docCount++;// 以128的BLOCK_SIZE写入docOut(.doc)文件中 if (docBufferUpto == BLOCK_SIZE) &#123; pforUtil.encode(docDeltaBuffer, docOut); if (writeFreqs) &#123; pforUtil.encode(freqBuffer, docOut); &#125; // NOTE: don&#x27;t set docBufferUpto back to 0 here; // finishDoc will do so (because it needs to see that // the block was filled so it can save skip data) &#125;// 更新lastDocID lastDocID = docID; lastPosition = 0; lastStartOffset = 0; long norm; if (fieldHasNorms) &#123; boolean found = norms.advanceExact(docID); if (found == false) &#123; // This can happen if indexing hits a problem after adding a doc to the // postings but before buffering the norm. Such documents are written // deleted and will go away on the first merge. norm = 1L; &#125; else &#123; norm = norms.longValue(); assert norm != 0 : docID; &#125; &#125; else &#123; norm = 1L; &#125; competitiveFreqNormAccumulator.add(writeFreqs ? termDocFreq : 1, norm); &#125; Lucene90PostingsWriter.finishDoc 12345678910111213141516171819public void finishDoc() throws IOException &#123; // Since we don&#x27;t know df for current term, we had to buffer // those skip data for each block, and when a new doc comes, // write them to skip file. if (docBufferUpto == BLOCK_SIZE) &#123; // skip data是针对block的，block对应的docId 是该block中最大的docId lastBlockDocID = lastDocID; if (posOut != null) &#123; if (payOut != null) &#123; lastBlockPayFP = payOut.getFilePointer(); &#125; lastBlockPosFP = posOut.getFilePointer(); lastBlockPosBufferUpto = posBufferUpto; lastBlockPayloadByteUpto = payloadByteUpto; &#125; docBufferUpto = 0; &#125; &#125; Lucene90PostingsWriter.finishTerm 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115 public void finishTerm(BlockTermState _state) throws IOException &#123; IntBlockTermState state = (IntBlockTermState) _state; assert state.docFreq &gt; 0; // TODO: wasteful we are counting this (counting # docs // for this term) in two places? assert state.docFreq == docCount : state.docFreq + &quot; vs &quot; + docCount; // docFreq == 1, don&#x27;t write the single docid/freq to a separate file along with a pointer to // it. final int singletonDocID; if (state.docFreq == 1) &#123; // pulse the singleton docid into the term dictionary, freq is implicitly totalTermFreq singletonDocID = (int) docDeltaBuffer[0]; &#125; else &#123; singletonDocID = -1; // vInt encode the remaining doc deltas and freqs: for (int i = 0; i &lt; docBufferUpto; i++) &#123; final int docDelta = (int) docDeltaBuffer[i]; final int freq = (int) freqBuffer[i]; if (!writeFreqs) &#123; // 将最后一个未满BLOCK_SIZE的block的docDelta 写入到doc文件中 docOut.writeVInt(docDelta); &#125; else if (freq == 1) &#123; docOut.writeVInt((docDelta &lt;&lt; 1) | 1); &#125; else &#123; docOut.writeVInt(docDelta &lt;&lt; 1); docOut.writeVInt(freq); &#125; &#125; &#125; final long lastPosBlockOffset; if (writePositions) &#123; // totalTermFreq is just total number of positions(or payloads, or offsets) // associated with current term. assert state.totalTermFreq != -1; if (state.totalTermFreq &gt; BLOCK_SIZE) &#123; // record file offset for last pos in last block lastPosBlockOffset = posOut.getFilePointer() - posStartFP; &#125; else &#123; lastPosBlockOffset = -1; &#125; if (posBufferUpto &gt; 0) &#123; // TODO: should we send offsets/payloads to // .pay...? seems wasteful (have to store extra // vLong for low (&lt; BLOCK_SIZE) DF terms = vast vast // majority) // vInt encode the remaining positions/payloads/offsets: int lastPayloadLength = -1; // force first payload length to be written int lastOffsetLength = -1; // force first offset length to be written int payloadBytesReadUpto = 0; for (int i = 0; i &lt; posBufferUpto; i++) &#123; final int posDelta = (int) posDeltaBuffer[i]; if (writePayloads) &#123; final int payloadLength = (int) payloadLengthBuffer[i]; if (payloadLength != lastPayloadLength) &#123; lastPayloadLength = payloadLength; posOut.writeVInt((posDelta &lt;&lt; 1) | 1); posOut.writeVInt(payloadLength); &#125; else &#123; posOut.writeVInt(posDelta &lt;&lt; 1); &#125; if (payloadLength != 0) &#123; posOut.writeBytes(payloadBytes, payloadBytesReadUpto, payloadLength); payloadBytesReadUpto += payloadLength; &#125; &#125; else &#123; posOut.writeVInt(posDelta); &#125; if (writeOffsets) &#123; int delta = (int) offsetStartDeltaBuffer[i]; int length = (int) offsetLengthBuffer[i]; if (length == lastOffsetLength) &#123; posOut.writeVInt(delta &lt;&lt; 1); &#125; else &#123; posOut.writeVInt(delta &lt;&lt; 1 | 1); posOut.writeVInt(length); lastOffsetLength = length; &#125; &#125; &#125; if (writePayloads) &#123; assert payloadBytesReadUpto == payloadByteUpto; payloadByteUpto = 0; &#125; &#125; &#125; else &#123; lastPosBlockOffset = -1; &#125; long skipOffset; // 写入skipList的data, 可以看到skipList的数据是写完posting之后再写入的 if (docCount &gt; BLOCK_SIZE) &#123; skipOffset = skipWriter.writeSkip(docOut) - docStartFP; &#125; else &#123; skipOffset = -1; &#125; state.docStartFP = docStartFP; state.posStartFP = posStartFP; state.payStartFP = payStartFP; state.singletonDocID = singletonDocID; state.skipOffset = skipOffset; state.lastPosBlockOffset = lastPosBlockOffset; docBufferUpto = 0; posBufferUpto = 0; lastDocID = 0; docCount = 0;&#125; Lucene90BlockTreeTermsWriter.TermsWriter.pushTerm 12345678910111213141516171819202122232425262728293031323334353637383940414243 private void pushTerm(BytesRef text) throws IOException &#123; // Find common prefix between last term and current term: int prefixLength = Arrays.mismatch( lastTerm.bytes(), 0, lastTerm.length(), text.bytes, text.offset, text.offset + text.length); if (prefixLength == -1) &#123; // Only happens for the first term, if it is empty assert lastTerm.length() == 0; prefixLength = 0; &#125; // if (DEBUG) System.out.println(&quot; shared=&quot; + pos + &quot; lastTerm.length=&quot; + lastTerm.length); // Close the &quot;abandoned&quot; suffix now: for (int i = lastTerm.length() - 1; i &gt;= prefixLength; i--) &#123; // How many items on top of the stack share the current suffix // we are closing: int prefixTopSize = pending.size() - prefixStarts[i]; if (prefixTopSize &gt;= minItemsInBlock) &#123; // if (DEBUG) System.out.println(&quot;pushTerm i=&quot; + i + &quot; prefixTopSize=&quot; + prefixTopSize + &quot; // minItemsInBlock=&quot; + minItemsInBlock); // 重点： 构建FST 相关的索引 writeBlocks(i + 1, prefixTopSize); prefixStarts[i] -= prefixTopSize - 1; &#125; &#125; if (prefixStarts.length &lt; text.length) &#123; prefixStarts = ArrayUtil.grow(prefixStarts, text.length); &#125; // Init new tail: for (int i = prefixLength; i &lt; text.length; i++) &#123; prefixStarts[i] = pending.size(); &#125; lastTerm.copyBytes(text);&#125; Lucene90BlockTreeTermsWriter.TermsWriter.writeBlocks 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124void writeBlocks(int prefixLength, int count) throws IOException &#123; assert count &gt; 0; // if (DEBUG2) &#123; // BytesRef br = new BytesRef(lastTerm.bytes()); // br.length = prefixLength; // System.out.println(&quot;writeBlocks: seg=&quot; + segment + &quot; prefix=&quot; + brToString(br) + &quot; count=&quot; // + count); // &#125; // Root block better write all remaining pending entries: assert prefixLength &gt; 0 || count == pending.size(); int lastSuffixLeadLabel = -1; // True if we saw at least one term in this block (we record if a block // only points to sub-blocks in the terms index so we can avoid seeking // to it when we are looking for a term): boolean hasTerms = false; boolean hasSubBlocks = false; int start = pending.size() - count; int end = pending.size(); int nextBlockStart = start; int nextFloorLeadLabel = -1; for (int i = start; i &lt; end; i++) &#123; PendingEntry ent = pending.get(i); int suffixLeadLabel; if (ent.isTerm) &#123; PendingTerm term = (PendingTerm) ent; if (term.termBytes.length == prefixLength) &#123; // Suffix is 0, i.e. prefix &#x27;foo&#x27; and term is // &#x27;foo&#x27; so the term has empty string suffix // in this block assert lastSuffixLeadLabel == -1 : &quot;i=&quot; + i + &quot; lastSuffixLeadLabel=&quot; + lastSuffixLeadLabel; suffixLeadLabel = -1; &#125; else &#123; suffixLeadLabel = term.termBytes[prefixLength] &amp; 0xff; &#125; &#125; else &#123; PendingBlock block = (PendingBlock) ent; assert block.prefix.length &gt; prefixLength; suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] &amp; 0xff; &#125; // if (DEBUG) System.out.println(&quot; i=&quot; + i + &quot; ent=&quot; + ent + &quot; suffixLeadLabel=&quot; + // suffixLeadLabel); if (suffixLeadLabel != lastSuffixLeadLabel) &#123; int itemsInBlock = i - nextBlockStart; if (itemsInBlock &gt;= minItemsInBlock &amp;&amp; end - nextBlockStart &gt; maxItemsInBlock) &#123; // The count is too large for one block, so we must break it into &quot;floor&quot; blocks, where // we record // the leading label of the suffix of the first term in each floor block, so at search // time we can // jump to the right floor block. We just use a naive greedy segmenter here: make a new // floor // block as soon as we have at least minItemsInBlock. This is not always best: it often // produces // a too-small block as the final block: boolean isFloor = itemsInBlock &lt; count; newBlocks.add( writeBlock( prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, i, hasTerms, hasSubBlocks)); hasTerms = false; hasSubBlocks = false; nextFloorLeadLabel = suffixLeadLabel; nextBlockStart = i; &#125; lastSuffixLeadLabel = suffixLeadLabel; &#125; if (ent.isTerm) &#123; hasTerms = true; &#125; else &#123; hasSubBlocks = true; &#125; &#125; // Write last block, if any: if (nextBlockStart &lt; end) &#123; int itemsInBlock = end - nextBlockStart; boolean isFloor = itemsInBlock &lt; count; newBlocks.add( writeBlock( prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, end, hasTerms, hasSubBlocks)); &#125; assert newBlocks.isEmpty() == false; PendingBlock firstBlock = newBlocks.get(0); assert firstBlock.isFloor || newBlocks.size() == 1; // NOTE: 每个block生成fst的index firstBlock.compileIndex(newBlocks, scratchBytes, scratchIntsRef); // Remove slice from the top of the pending stack, that we just wrote: pending.subList(pending.size() - count, pending.size()).clear(); // Append new block pending.add(firstBlock); newBlocks.clear();&#125; Lucene90BlockTreeTermsWriter.TermsWriter.writeBlockPendingBlock中记录了对应的在.tim文件中的位置(startFP), 后续会被FST索引到对应的value里 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124private PendingBlock writeBlock( int prefixLength, boolean isFloor, int floorLeadLabel, int start, int end, boolean hasTerms, boolean hasSubBlocks) throws IOException &#123; // .... // if (isLeafBlock) &#123; subIndices = null; StatsWriter statsWriter = new StatsWriter(this.statsWriter, fieldInfo.getIndexOptions() != IndexOptions.DOCS); for (int i = start; i &lt; end; i++) &#123; PendingEntry ent = pending.get(i); assert ent.isTerm : &quot;i=&quot; + i; PendingTerm term = (PendingTerm) ent; assert StringHelper.startsWith(term.termBytes, prefix) : term + &quot; prefix=&quot; + prefix; BlockTermState state = term.state; final int suffix = term.termBytes.length - prefixLength; // if (DEBUG2) &#123; // BytesRef suffixBytes = new BytesRef(suffix); // System.arraycopy(term.termBytes, prefixLength, suffixBytes.bytes, 0, suffix); // suffixBytes.length = suffix; // System.out.println(&quot; write term suffix=&quot; + brToString(suffixBytes)); // &#125; // For leaf block we write suffix straight suffixLengthsWriter.writeVInt(suffix); suffixWriter.append(term.termBytes, prefixLength, suffix); assert floorLeadLabel == -1 || (term.termBytes[prefixLength] &amp; 0xff) &gt;= floorLeadLabel; // Write term stats, to separate byte[] blob: statsWriter.add(state.docFreq, state.totalTermFreq); // Write term meta data postingsWriter.encodeTerm(metaWriter, fieldInfo, state, absolute); absolute = false; &#125; &#125; else &#123; // Block has at least one prefix term or a sub block: subIndices = new ArrayList&lt;&gt;(); StatsWriter statsWriter = new StatsWriter(this.statsWriter, fieldInfo.getIndexOptions() != IndexOptions.DOCS); for (int i = start; i &lt; end; i++) &#123; PendingEntry ent = pending.get(i); if (ent.isTerm) &#123; PendingTerm term = (PendingTerm) ent; assert StringHelper.startsWith(term.termBytes, prefix) : term + &quot; prefix=&quot; + prefix; BlockTermState state = term.state; final int suffix = term.termBytes.length - prefixLength; // For non-leaf block we borrow 1 bit to record // if entry is term or sub-block, and 1 bit to record if // it&#x27;s a prefix term. Terms cannot be larger than ~32 KB // so we won&#x27;t run out of bits: suffixLengthsWriter.writeVInt(suffix &lt;&lt; 1); suffixWriter.append(term.termBytes, prefixLength, suffix); // Write term stats, to separate byte[] blob: statsWriter.add(state.docFreq, state.totalTermFreq); // TODO: now that terms dict &quot;sees&quot; these longs, // we can explore better column-stride encodings // to encode all long[0]s for this block at // once, all long[1]s, etc., e.g. using // Simple64. Alternatively, we could interleave // stats + meta ... no reason to have them // separate anymore: // Write term meta data postingsWriter.encodeTerm(metaWriter, fieldInfo, state, absolute); absolute = false; &#125; else &#123; PendingBlock block = (PendingBlock) ent; assert StringHelper.startsWith(block.prefix, prefix); final int suffix = block.prefix.length - prefixLength; assert StringHelper.startsWith(block.prefix, prefix); assert suffix &gt; 0; // For non-leaf block we borrow 1 bit to record // if entry is term or sub-block:f suffixLengthsWriter.writeVInt((suffix &lt;&lt; 1) | 1); suffixWriter.append(block.prefix.bytes, prefixLength, suffix); // if (DEBUG2) &#123; // BytesRef suffixBytes = new BytesRef(suffix); // System.arraycopy(block.prefix.bytes, prefixLength, suffixBytes.bytes, 0, suffix); // suffixBytes.length = suffix; // System.out.println(&quot; write sub-block suffix=&quot; + brToString(suffixBytes) + &quot; // subFP=&quot; + block.fp + &quot; subCode=&quot; + (startFP-block.fp) + &quot; floor=&quot; + block.isFloor); // &#125; assert floorLeadLabel == -1 || (block.prefix.bytes[prefixLength] &amp; 0xff) &gt;= floorLeadLabel : &quot;floorLeadLabel=&quot; + floorLeadLabel + &quot; suffixLead=&quot; + (block.prefix.bytes[prefixLength] &amp; 0xff); assert block.fp &lt; startFP; suffixLengthsWriter.writeVLong(startFP - block.fp); subIndices.add(block.index); &#125; &#125; statsWriter.finish(); assert subIndices.size() != 0; &#125; // ... // Write term meta data byte[] blob termsOut.writeVInt((int) metaWriter.size()); // 写入到`tim`文件 metaWriter.copyTo(termsOut); metaWriter.reset();&#125; Lucene90PostingWriter.encodeTerm 12345678910111213141516171819202122232425262728293031323334353637383940414243// 这里的DataOutput 指的是ByteBuffersDataOutput, 一个内存结构, 这里记录了每个Term 倒排索引对应的文件位置相关, 最终会被写入`.tim`文件public void encodeTerm( DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException &#123; IntBlockTermState state = (IntBlockTermState) _state; if (absolute) &#123; lastState = emptyState; assert lastState.docStartFP == 0; &#125; if (lastState.singletonDocID != -1 &amp;&amp; state.singletonDocID != -1 &amp;&amp; state.docStartFP == lastState.docStartFP) &#123; // With runs of rare values such as ID fields, the increment of pointers in the docs file is // often 0. // Furthermore some ID schemes like auto-increment IDs or Flake IDs are monotonic, so we // encode the delta // between consecutive doc IDs to save space. final long delta = (long) state.singletonDocID - lastState.singletonDocID; out.writeVLong((BitUtil.zigZagEncode(delta) &lt;&lt; 1) | 0x01); &#125; else &#123; out.writeVLong((state.docStartFP - lastState.docStartFP) &lt;&lt; 1); if (state.singletonDocID != -1) &#123; out.writeVInt(state.singletonDocID); &#125; &#125; if (writePositions) &#123; out.writeVLong(state.posStartFP - lastState.posStartFP); if (writePayloads || writeOffsets) &#123; out.writeVLong(state.payStartFP - lastState.payStartFP); &#125; &#125; if (writePositions) &#123; if (state.lastPosBlockOffset != -1) &#123; out.writeVLong(state.lastPosBlockOffset); &#125; &#125; if (state.skipOffset != -1) &#123; out.writeVLong(state.skipOffset); &#125; lastState = state; &#125; Lucene90BlockTreeTermsWriter.TermsWriter.finish 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public void finish() throws IOException &#123; if (numTerms &gt; 0) &#123; // if (DEBUG) System.out.println(&quot;BTTW: finish prefixStarts=&quot; + // Arrays.toString(prefixStarts)); // Add empty term to force closing of all final blocks: pushTerm(new BytesRef()); // TODO: if pending.size() is already 1 with a non-zero prefix length // we can save writing a &quot;degenerate&quot; root block, but we have to // fix all the places that assume the root block&#x27;s prefix is the empty string: pushTerm(new BytesRef()); // 这里完成将所有Term生成FST的索引, pending.get(0)是root的block，其索引包含了整个pending的Terms writeBlocks(0, pending.size()); // We better have one final &quot;root&quot; block: assert pending.size() == 1 &amp;&amp; !pending.get(0).isTerm : &quot;pending.size()=&quot; + pending.size() + &quot; pending=&quot; + pending; final PendingBlock root = (PendingBlock) pending.get(0); assert root.prefix.length == 0; final BytesRef rootCode = root.index.getEmptyOutput(); assert rootCode != null; ByteBuffersDataOutput metaOut = new ByteBuffersDataOutput(); fields.add(metaOut); metaOut.writeVInt(fieldInfo.number); metaOut.writeVLong(numTerms); metaOut.writeVInt(rootCode.length); metaOut.writeBytes(rootCode.bytes, rootCode.offset, rootCode.length); assert fieldInfo.getIndexOptions() != IndexOptions.NONE; if (fieldInfo.getIndexOptions() != IndexOptions.DOCS) &#123; metaOut.writeVLong(sumTotalTermFreq); &#125; metaOut.writeVLong(sumDocFreq); metaOut.writeVInt(docsSeen.cardinality()); writeBytesRef(metaOut, new BytesRef(firstPendingTerm.termBytes)); writeBytesRef(metaOut, new BytesRef(lastPendingTerm.termBytes)); metaOut.writeVLong(indexOut.getFilePointer()); // Write FST to index // 这里的indexOut就是&quot;.tip&quot; root.index.save(metaOut, indexOut); // System.out.println(&quot; write FST &quot; + indexStartFP + &quot; field=&quot; + fieldInfo.name); /* if (DEBUG) &#123; final String dotFileName = segment + &quot;_&quot; + fieldInfo.name + &quot;.dot&quot;; Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName)); Util.toDot(root.index, w, false, false); System.out.println(&quot;SAVED to &quot; + dotFileName); w.close(); &#125; */ &#125; else &#123; assert sumTotalTermFreq == 0 || fieldInfo.getIndexOptions() == IndexOptions.DOCS &amp;&amp; sumTotalTermFreq == -1; assert sumDocFreq == 0; assert docsSeen.cardinality() == 0; &#125; &#125; 总结Lucene整体倒排索引设计还是非常复杂的，这里仅仅是将内存的数据写入到磁盘就需要这么多步骤，同时Lucene保持了很好的抽象，不同格式的索引数据只要涉及实现对应的接口即可，通过codec的方式嵌入到引擎中. 我的问题是如何评估这些设计对性能的影响，FST的涉及需要如此复杂吗，只是为了定位Term对应的倒排表位置，有没有其他可以使用的数据结构呢, 在后续的学习过程中还是要多加深入了解和思考，希望能得到更好的答案.","categories":[],"tags":[{"name":"ES, Lucene, elasticsearch","slug":"ES-Lucene-elasticsearch","permalink":"https://skyitachi.github.io/tags/ES-Lucene-elasticsearch/"}]},{"title":"IVFPQ的原理","slug":"IVFPQ的原理","date":"2024-08-03T16:00:00.000Z","updated":"2024-08-03T16:00:00.000Z","comments":true,"path":"2024/08/04/IVFPQ的原理/","permalink":"https://skyitachi.github.io/2024/08/04/IVFPQ%E7%9A%84%E5%8E%9F%E7%90%86/","excerpt":"","text":"主要内容 介绍IVF和PQ的简单原理 介绍IVFPQ的主要流程 Kmeans 聚类算法在IVF和PQ中都涉及到聚类算法，简单介绍下kmeans聚类算法原理 12341. 随机选择k个初始中心点2. 将每个向量分配到最近的中心点所在的簇3. 重新计算每个簇的中心点（找出质心）4. 重复步骤2和3，直到簇中心不再变化或者达到迭代次数 IVF的与原理介绍 Inverted File (IVF) 是一种索引结构，通常用于信息检索系统，特别是在全文搜索引擎中 IVF的原理简单直接: 向量中的IVF索引就是将样本向量首先生成nlist个聚类，然后每次向量查询的时候只要比较查询向量和nlist个聚类中心的距离，可以找出nprobe个最近的聚类中心，然后在每个聚类里暴力搜索出距离最近的K个向量 代码示例123456789101112131415161718192021222324252627282930import numpy as npimport faiss# 生成一些示例数据d = 64 # 向量维度nb = 100000 # 数据库大小nq = 10 # 查询数量np.random.seed(1234) # 确保可重复性xb = np.random.random((nb, d)).astype(&#x27;float32&#x27;)xq = np.random.random((nq, d)).astype(&#x27;float32&#x27;)# 构建 IVF 索引nlist = 1024 # 聚类数量quantizer = faiss.IndexFlatL2(d) # 量化器index = faiss.IndexIVFFlat(quantizer, d, nlist)# 训练索引index.train(xb)index.add(xb)index.nprobe = 256# 查询k = 4 # 返回的最近邻数量D, I = index.search(xq, k) # 搜索# 打印结果print(&quot;查询结果 (距离, 索引):&quot;)for i in range(nq): print(f&quot;查询 &#123;i&#125; PQ(Product Quantization)的原理与介绍PQ 主要用于向量的压缩, 一般而言向量都是高维数据, 假设一个1024维的float32类型的向量，单个向量的大小就是1024 * 4 &#x3D; 4kb, 使用PQ压缩后往往能极大缩小单个向量的大小，当然不可避免的会丢失精度。 PQ向量压缩的主要步骤121. 将d维向量分成M组，在每一组内找到ksub个聚类中心2. 将每一组的向量用离它最近聚类中心的id来表示，那么每组内的向量的大小就是nbits（2^nbits=ksub）, 整体压缩过后的向量大小就是M * nbits, 一般而言M选择8，nbits一般是8或16 搜索压缩过后的样本向量1231. 将查询向量分为M组，每一组内和ksub个聚类中心计算距离, 并使用一张表存下来(table[i][ck])2. 计算查询向量和样本向量的距离时，可以用距离聚类中心的距离来代替，进行O(1)的查表即可获取，（样本空间压缩后的编码是[c0, c1, c2... cM-1]，只要取之前计算好的table[i][ck]的距离即可）3. 取最近K个即可 IVFPQ的工作原理1234567891011# 建立索引1. 样本空间先用IVF 分类成nlist个聚类2. 每个聚类中的样本向量计算它们和聚类中心的差值，得到新的向量3. 使用PQ压缩每个聚类中新的样本向量# 查询1. 先找到距离查询量最近的聚类中心2. 计算查询向量和聚类中心的差值，得到新的查询向量3. 对新的查询向量使用PQ量化，并和该聚类中的压缩过后的样本向量做距离计算，并取出K个 代码示例123456789101112131415161718192021222324252627282930import numpy as npimport faiss# 生成一些示例数据d = 128 # 向量维度nb = 100000 # 数据库大小nq = 10 # 查询数量np.random.seed(1234) # 确保可重复性xb = np.random.random((nb, d)).astype(&#x27;float32&#x27;)xq = np.random.random((nq, d)).astype(&#x27;float32&#x27;)# 构建IVFPQ索引nlist = 100 # 聚类数量, ivf的聚类m = 8 # 每个向量的子向量数量nbit = 8 # ksub = 2^8, 每组要有ksub个聚类中心quantizer = faiss.IndexFlatL2(d) # 量化器index = faiss.IndexIVFPQ(quantizer, d, nlist, m, nbit) # 8位量化# 训练索引index.train(xb)index.add(xb)# 查询k = 4 # 返回的最近邻数量D, I = index.search(xq, k) # 搜索# 打印结果print(&quot;查询结果 (距离, 索引):&quot;)for i in range(nq): print(f&quot;查询 &#123;i&#125;: 总结PQ量化压缩技术在向量索引领域较为常见，了解其整体的工作流程有助于我们选择合适的向量索引，从而达到较好的搜索性能. 而且PQ可以和很多算法结合，可以帮助理解部分faiss中index_factory的参数","categories":[],"tags":[{"name":"algorithm 向量数据库 vector ann","slug":"algorithm-向量数据库-vector-ann","permalink":"https://skyitachi.github.io/tags/algorithm-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-vector-ann/"}]},{"title":"java中如何处理批量请求及其异常处理","slug":"java中的线程池异常处理","date":"2024-07-25T16:00:00.000Z","updated":"2024-07-25T16:00:00.000Z","comments":true,"path":"2024/07/26/java中的线程池异常处理/","permalink":"https://skyitachi.github.io/2024/07/26/java%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","excerpt":"","text":"场景1234567891011121314151617181920public class ScoredItems &#123; public Object items; public double score; public ScoredItems(Object items, double score) &#123; this.items = items; this.score = score; &#125;&#125;public interface Scorer &#123; List&lt;ScoredItems&gt; score(List&lt;Object&gt; items);&#125;public class Ranker &#123; private Scorer scorer; public List&lt;ScoredItems&gt; rank(List&lt;Object&gt; items) &#123; // TODO: &#125;&#125; 问题请实现Ranker.rank接口，并以全局score降序排列返回List&lt;ScoredItems&gt; 其中items的长度最大是10000， Scorer.score接口items的长度最大是100. 实现一使用线程池处理批量请求1234567891011121314151617181920212223242526 public List&lt;ScoredItems&gt; rank(List&lt;Object&gt; items) &#123; // 100 分组 List&lt;List&lt;Object&gt;&gt; partitions = Lists.partition(items, 100); if (partitions.size() == 1) &#123; return scorer.score(items); &#125; List&lt;Future&lt;List&lt;ScoredItems&gt;&gt;&gt; futures = new ArrayList&lt;&gt;(); for(List&lt;Object&gt; partition: partitions) &#123; Future&lt;List&lt;ScoredItems&gt;&gt; future = threadPoolExecutor.submit(() -&gt; scorer.score(partition)); futures.add(future); &#125; List&lt;ScoredItems&gt; results = new ArrayList&lt;&gt;(); for (Future&lt;List&lt;ScoredItems&gt;&gt; f: futures) &#123; List&lt;ScoredItems&gt; result = f.get(); results.addAll(result); &#125; results.sort((o1, o2) -&gt; Double.compare(o2.score, o1.score)); return results;&#125; 其中线程池的初始化 1threadPoolExecutor = new ThreadPoolExecutor(100, 100, 60, TimeUnit.SECONDS, workQueue); 说明 基于分组，然后利用线城池批量请求, 达到并发的的目的 缺点: 1. 没有异常处理, 2. 线程池的设置合理性 实现二加入异常处理并对超时异常特殊处理, 异常处理的准则是只要有一个score请求发生异常了，这个rank接口需要抛异常12345678910for (Future&lt;List&lt;ScoredItems&gt;&gt; f: futures) &#123; try &#123; List&lt;ScoredItems&gt; result = f.get(timeout, TimeUnit.MILLISECONDS); results.addAll(result); &#125; catch (TimeoutException e) &#123; throw new RPCTimeoutException(&quot;超时异常&quot;); &#125; catch (ExecutionException | InterruptedException e) &#123; throw new SystemException(&quot;其他异常&quot;); &#125;&#125; 说明 这里在循环里用了future.get(timeout, TimeUnit.MILLISECONDS)造成了整体超时时间会累积，我们期望的是接口每一次调用的延迟是固定的. 这种写法是有问题的. 这里只处理了发生异常的请求, rank接口层面已经完成了异常处理，但是其他请求仍然在线程池里执行 通过future.cancel 会取消其他线程池里的执行线程吗 (只适用于非超时异常的场景, 如果发生了超时异常再去取消，那么其他future大概率要么完成，要么已经异常) 实现三123456789101112131415161718192021222324252627282930313233public List&lt;ScoredItems&gt; rank(List&lt;Object&gt; items) throws RPCTimeoutException &#123; List&lt;List&lt;Object&gt;&gt; partitions = Lists.partition(items, 100); if (partitions.size() == 1) &#123; return scorer.score(items); &#125; List&lt;CompletableFuture&lt;List&lt;ScoredItems&gt;&gt;&gt; allScored = partitions .stream() .map(partition -&gt; CompletableFuture.supplyAsync(() -&gt; scorer.score(partition), executor) .orTimeout(timeout, TimeUnit.MILLISECONDS)) .toList(); List&lt;ScoredItems&gt; result = new ArrayList&lt;&gt;(); for(CompletableFuture&lt;List&lt;ScoredItems&gt;&gt; pf: allScored) &#123; try &#123; List&lt;ScoredItems&gt; scored = pf.join(); result.addAll(scored); &#125; catch (CompletionException e) &#123; if (e.getCause() instanceof TimeoutException) &#123; logger.warn(&quot;[rank] found TimeoutException&quot;); throw new RPCTimeoutException(e.getMessage()); &#125; else &#123; // pass &#125; &#125; catch (CancellationException e) &#123; // pass &#125; &#125; result.sort((o1, o2) -&gt; Double.compare(o2.score, o1.score)); return result;&#125; 说明 使用了CompletableFuture.orTimeout 处理每个请求的异常，获取结果的时候用了cf.join() 方法. 使用CompletableFuture总是更好的选择 CompletableFuture.cancel(true)的实际效果是什么, 会节省线程池资源吗 CompletableFuture.cancel(true)的实验12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static void groupCompletableFutureDemo() &#123; List&lt;CompletableFuture&lt;Integer&gt;&gt; futures = Stream.of(1, 2) .map(o -&gt; CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; Thread.sleep(1000); logger.info(&quot;[GroupCompletableFuture] finish execution got result &#123;&#125;&quot;, o * 2); return o * 2; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125;, gExecutorService).orTimeout(o * 900, TimeUnit.MILLISECONDS)) .toList(); int timeoutExceptions = 0; int cancelExceptions = 0; for(CompletableFuture&lt;Integer&gt; future: futures) &#123; try &#123; Integer result = future.join(); logger.info(&quot;[GroupCompletableFuture] got result: &#123;&#125;&quot;, result); &#125; catch (CompletionException e) &#123; if (e.getCause() instanceof TimeoutException) &#123; timeoutExceptions += 1; logger.warn(&quot;[GroupCompletableFuture] got TimeoutException correctly done: &#123;&#125;&quot;, future.isDone()); for (CompletableFuture&lt;Integer&gt; f: futures) &#123; if (!f.isDone() &amp;&amp; !f.isCancelled()) &#123; f.cancel(true); &#125; &#125; &#125; &#125; catch (CancellationException e) &#123; cancelExceptions += 1; logger.warn(&quot;[GroupCompletableFuture] got cancel exception&quot;); &#125; &#125; logger.info(&quot;[GroupCompletableFuture] timeout exceptions: &#123;&#125;, cancel exceptions: &#123;&#125;&quot;, timeoutExceptions, cancelExceptions);&#125;// 2024-07-26 11:48:48 WARN ConcurrentScoreDemo:434 - [GroupCompletableFuture] got TimeoutException correctly done: true2024-07-26 11:48:48 WARN ConcurrentScoreDemo:443 - [GroupCompletableFuture] got cancel exception2024-07-26 11:48:48 INFO ConcurrentScoreDemo:447 - [GroupCompletableFuture] timeout exceptions: 1, cancel exceptions: 12024-07-26 11:48:48 INFO ConcurrentScoreDemo:415 - [GroupCompletableFuture] finish execution got result 42024-07-26 11:48:48 INFO ConcurrentScoreDemo:415 - [GroupCompletableFuture] finish execution got result 2 说明 CompletableFuture.cancel(true) 并不能取消线程的执行，只会更改future本身的状态 使用Thread.interrupt() 改变Thread运行状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public static void longRunningTask(ScheduledExecutorService scheduledExecutorService, long timeout) throws InterruptedException &#123; Thread thread = Thread.currentThread(); AtomicBoolean done = new AtomicBoolean(false); scheduledExecutorService.schedule(new Runnable() &#123; @Override public void run() &#123; if (!done.get()) &#123; done.set(true); thread.interrupt(); &#125; &#125; &#125;, timeout, TimeUnit.MILLISECONDS); // 模拟耗时工作 Thread.sleep(1000);&#125;public static void realInterruptTaskDemo() throws InterruptedException &#123; ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); BlockingQueue&lt;Runnable&gt; workingQueue = new LinkedBlockingQueue&lt;&gt;(Integer.MAX_VALUE); ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 10, 60, TimeUnit.MILLISECONDS, workingQueue); long start = System.currentTimeMillis(); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123; try &#123; longRunningTask(scheduledExecutorService, 800); logger.info(&quot;[realInterruptTaskDemo.longRunningTask] finished&quot;); return 1; &#125; catch (InterruptedException e) &#123; logger.info(&quot;[realInterruptTaskDemo.longRunningTask] canceled: &#123;&#125;&quot;, e.getMessage()); &#125; return null; &#125;, executor); logger.warn(&quot;[realInterruptTaskDemo] active thread count: &#123;&#125;&quot;, executor.getActiveCount()); try &#123; future.join(); &#125; catch (CompletionException e) &#123; logger.warn(&quot;[realInterruptTaskDemo] CompletionException: &#123;&#125;&quot;, e.getMessage()); &#125; catch (CancellationException e) &#123; logger.warn(&quot;[realInterruptTaskDemo] CancellationException: &#123;&#125;&quot;, e.getMessage()); &#125; logger.warn(&quot;[realInterruptTaskDemo] task is still running after join active thread count: &#123;&#125;, consumes: &#123;&#125;&quot;, executor.getActiveCount(), System.currentTimeMillis() - start); if (future.isCompletedExceptionally()) &#123; logger.warn(&quot;[realInterruptTaskDemo] future is error&quot;); &#125; scheduledExecutorService.shutdown(); executor.shutdown();&#125;// output2024-07-26 11:59:50 WARN ConcurrentScoreDemo:281 - [realInterruptTaskDemo] active thread count: 12024-07-26 11:59:51 INFO ConcurrentScoreDemo:276 - [realInterruptTaskDemo.longRunningTask] canceled: sleep interrupted2024-07-26 11:59:51 WARN ConcurrentScoreDemo:291 - [realInterruptTaskDemo] task is still running after join active thread count: 0, consumes: 803 说明 使用了Thread.interrupt之后，本来1000ms的sleep 在800ms的时候被打断了，同时线程也没了. 这样做的后果是什么, 虽然activeCount的线程没了，但是意味着线程池需要重新创建一个新的线程（有必要的情况下），创建线程同样也需要消耗资源, 实际的生产环境中，一般都是有超时处理的，这种情况直接让请求在线程池中走完，而不是销毁线程，这样更加合理一些. 总结 在处理批量的请求中，异常处理使用CompleableFuture更加合理和简洁，接口的超时使用orTimeout加入超时机制，避免在循环中手动调用future.get(timeout)这种实践","categories":[],"tags":[{"name":"java, Exception, CompletableFuture, ThreadPool, batch","slug":"java-Exception-CompletableFuture-ThreadPool-batch","permalink":"https://skyitachi.github.io/tags/java-Exception-CompletableFuture-ThreadPool-batch/"}]},{"title":"ann-benchmarks中hnsw简单解读","slug":"hnswlib和faiss_hnsw的ann-benchmarks结果解读","date":"2024-07-22T16:00:00.000Z","updated":"2024-07-22T16:00:00.000Z","comments":true,"path":"2024/07/23/hnswlib和faiss_hnsw的ann-benchmarks结果解读/","permalink":"https://skyitachi.github.io/2024/07/23/hnswlib%E5%92%8Cfaiss_hnsw%E7%9A%84ann-benchmarks%E7%BB%93%E6%9E%9C%E8%A7%A3%E8%AF%BB/","excerpt":"","text":"hnswlib和faiss_hnsw在ann-benchmark中的参数解读benchmark链接只需要关注hnswlib和hnsw(faiss) 即可 声明 数据集: sift-128-euclidean数据集(向量的维度是128) k-nn的k是10 hnswlib和faiss_hnsw的benchmark都是基于单线程的 benchmark的细节点说明 hnswlib中有相同图例的点, eg：Parameters: hnswlib (&#123;&#39;M&#39;: 12, &#39;efConstruction&#39;: 500&#125;) 这样得点标记了好几个，但是QPS和Recall都不相同,原因在于hnswlib的benchmark配置中有query-args参数 1234567891011121314151617181920// ann_benchmarks/algorithms/hnswlib/config.ymlfloat: any: - base_args: [&#x27;@metric&#x27;] constructor: HnswLib disabled: false docker_tag: ann-benchmarks-hnswlib module: ann_benchmarks.algorithms.hnswlib name: hnswlib run_groups: M-12: arg_groups: [&#123;M: 12, efConstruction: 500&#125;] args: &#123;&#125; query_args: [[10, 20, 40, 80, 120, 200, 400, 600, 800]] ...// query_args 对应了 hnswlib/module.py中的set_query_arguments函数的参数 def set_query_arguments(self, ef): self.p.set_ef(ef) hnswlib中set_ef和faiss_hnsw中设置efSearch的效果是一样的, faiss中的图例就标注了efSearch(ef) ,&quot;faiss (&#123;&#39;M&#39;: 12, &#39;efConstruction&#39;: 500&#125;, ef: 80) efSearch不影响build，所以在Recall&#x2F;Build time的图上hnswlib就没有”重复”的点了 hnswlib和faiss_hnsw的性能表现主要看recall和build的表现 ann-benchmark中的表现参考ann-benchmark的论文，他们运行的硬件环境是: Amazon EC2 c5.4xlarge instances that are equipped with Intel Xeon Platinum 8124M CPU (16 cores available, 3.00 GHz, 25.0MB Cache) and 32GB of RAM running Amazon Linux. ann-benchmarks网站上的benchmark如下: 说明: 整体QPS随着recall的上升成指数级的下降 (两者都是) 相同召回率的情况下，hnswlib的QPS要高一点, 由于faiss标注了ef(efSearch)但是hnswlib没有标注，所以比较起来不太直观，后面我在本地自己跑的时候给hnswlib带上了ef参数，这样就更加直观了 说明: 1. Build Index Time也是hnswlib占优 本地的表现硬件环境: AMD Ryzen 5 3600 (3.6G Hz), 6 core 12 threads, 32GB of RAM running Ubuntu 24.04 这里我修改了ann-benchmarks中hnswlib的环境和在efConstruction&#x3D;200和500的情况下比较hnswlib的表现 123456789// ann_benchmarks/algorithms/hnswlib/config.ymlFROM ann-benchmarks# RUN apt-get install -y python-setuptools python-pipRUN pip3 install pybind11 numpy setuptools hnswlib==0.8.0# RUN cd hnsw/python_bindings; python3 setup.py installRUN python3 -c &#x27;import hnswlib&#x27; Recall &amp; QPS 表现关于recall 我们更需要关注是否通过参数配置到足够高的精度，我这里以0.99的recall作为基础，主要比较在0.99以上的召回率条件下(qps &gt; 2000)，两种算法实现的qps及其对应的参数 algorithm parameters k-nn(recall) qps build(s) indexsize(kb) faiss_hnsw faiss ({‘M’: 24, ‘efConstruction’: 500}, ef: 80) 0.99031 2561.923162412995 1458.773098230362 794864.0 faiss_hnsw faiss ({‘M’: 16, ‘efConstruction’: 500}, ef: 120) 0.99347 2194.419076982479 1285.849939107895 732492.0 faiss_hnsw faiss ({‘M’: 36, ‘efConstruction’: 500}, ef: 80) 0.99314 2191.7604350595634 1585.1033165454865 888936.0 faiss_hnsw faiss ({‘M’: 48, ‘efConstruction’: 500}, ef: 80) 0.99411 2077.6779248107487 1667.5446255207062 982148.0 hnswlib hnswlib ({‘M’: 64, ‘efConstruction’: 200}, ef: 80) 0.9901500000000001 3731.2651844102425 496.65849924087524 1150800.0 hnswlib hnswlib ({‘M’: 96, ‘efConstruction’: 200}, ef: 80) 0.9902599999999999 3616.44180507205 513.7249546051025 1400632.0 hnswlib hnswlib ({‘M’: 36, ‘efConstruction’: 500}, ef: 80) 0.99308 3571.6658501707066 1156.385510444641 931660.0 hnswlib hnswlib ({‘M’: 24, ‘efConstruction’: 200}, ef: 120) 0.9939 3314.9200264352426 426.75320744514465 838564.0 hnswlib hnswlib ({‘M’: 48, ‘efConstruction’: 500}, ef: 80) 0.99433 3290.7135482210724 1246.9214706420898 1026148.0 hnswlib hnswlib ({‘M’: 64, ‘efConstruction’: 500}, ef: 80) 0.9946999999999999 3153.6867226554614 1276.8294219970703 1146332.0 hnswlib hnswlib ({‘M’: 24, ‘efConstruction’: 500}, ef: 120) 0.9955999999999999 3139.927141602247 1008.0483357906342 838676.0 hnswlib hnswlib ({‘M’: 12, ‘efConstruction’: 200}, ef: 200) 0.9920199999999999 3100.2119285498034 313.95580410957336 745664.0 hnswlib hnswlib ({‘M’: 12, ‘efConstruction’: 500}, ef: 200) 0.99271 3040.3355289720653 709.9178235530853 745132.0 hnswlib hnswlib ({‘M’: 96, ‘efConstruction’: 500}, ef: 80) 0.9949 3036.296166689699 1317.8283751010895 1400940.0 hnswlib hnswlib ({‘M’: 36, ‘efConstruction’: 200}, ef: 120) 0.99544 2873.9128879689624 466.08784890174866 931436.0 hnswlib hnswlib ({‘M’: 48, ‘efConstruction’: 200}, ef: 120) 0.9956799999999999 2690.4460742892884 482.9417383670807 1025096.0 hnswlib hnswlib ({‘M’: 64, ‘efConstruction’: 200}, ef: 120) 0.99576 2671.663602183599 496.65849924087524 1150800.0 hnswlib hnswlib ({‘M’: 36, ‘efConstruction’: 500}, ef: 120) 0.99753 2612.247989122775 1156.385510444641 931660.0 hnswlib hnswlib ({‘M’: 96, ‘efConstruction’: 200}, ef: 120) 0.99588 2581.161300550079 513.7249546051025 1400632.0 hnswlib hnswlib ({‘M’: 16, ‘efConstruction’: 200}, ef: 200) 0.9962 2565.1980331237614 364.242990732193 778360.0 hnswlib hnswlib ({‘M’: 48, ‘efConstruction’: 500}, ef: 120) 0.9979100000000001 2394.6802442922076 1246.9214706420898 1026148.0 hnswlib hnswlib ({‘M’: 64, ‘efConstruction’: 500}, ef: 120) 0.9980800000000001 2282.691070497442 1276.8294219970703 1146332.0 hnswlib hnswlib ({‘M’: 96, ‘efConstruction’: 500}, ef: 120) 0.99821 2188.4360442789643 1317.8283751010895 1400940.0 hnswlib hnswlib ({‘M’: 24, ‘efConstruction’: 200}, ef: 200) 0.99818 2166.896152958348 426.75320744514465 838564.0 hnswlib hnswlib ({‘M’: 8, ‘efConstruction’: 200}, ef: 400) 0.99244 2138.951169866413 256.2939279079437 715140.0 hnswlib hnswlib ({‘M’: 8, ‘efConstruction’: 500}, ef: 400) 0.9936 2124.6338197635687 588.7308986186981 715372.0 hnswlib hnswlib ({‘M’: 24, ‘efConstruction’: 500}, ef: 200) 0.9989100000000001 2017.995520316913 1008.0483357906342 838676.0 Build Index Time algorithm parameters indexsize build hnswlib hnswlib ({‘M’: 8, ‘efConstruction’: 200}, ef: 400) 715140.0 256.2939279079437 hnswlib hnswlib ({‘M’: 12, ‘efConstruction’: 200}, ef: 200) 745664.0 313.95580410957336 hnswlib hnswlib ({‘M’: 16, ‘efConstruction’: 200}, ef: 200) 778360.0 364.242990732193 hnswlib hnswlib ({‘M’: 24, ‘efConstruction’: 200}, ef: 120) 838564.0 426.75320744514465 hnswlib hnswlib ({‘M’: 36, ‘efConstruction’: 200}, ef: 120) 931436.0 466.08784890174866 hnswlib hnswlib ({‘M’: 48, ‘efConstruction’: 200}, ef: 120) 1025096.0 482.9417383670807 hnswlib hnswlib ({‘M’: 64, ‘efConstruction’: 200}, ef: 120) 1150800.0 496.65849924087524 hnswlib hnswlib ({‘M’: 96, ‘efConstruction’: 200}, ef: 80) 1400632.0 513.7249546051025 hnswlib hnswlib ({‘M’: 8, ‘efConstruction’: 500}, ef: 400) 715372.0 588.7308986186981 hnswlib hnswlib ({‘M’: 12, ‘efConstruction’: 500}, ef: 200) 745132.0 709.9178235530853 hnswlib hnswlib ({‘M’: 24, ‘efConstruction’: 500}, ef: 120) 838676.0 1008.0483357906342 hnswlib hnswlib ({‘M’: 36, ‘efConstruction’: 500}, ef: 120) 931660.0 1156.385510444641 hnswlib hnswlib ({‘M’: 48, ‘efConstruction’: 500}, ef: 120) 1026148.0 1246.9214706420898 hnswlib hnswlib ({‘M’: 64, ‘efConstruction’: 500}, ef: 120) 1146332.0 1276.8294219970703 faiss_hnsw faiss ({‘M’: 16, ‘efConstruction’: 500}, ef: 120) 732492.0 1285.849939107895 hnswlib hnswlib ({‘M’: 96, ‘efConstruction’: 500}, ef: 120) 1400940.0 1317.8283751010895 faiss_hnsw faiss ({‘M’: 24, ‘efConstruction’: 500}, ef: 80) 794864.0 1458.773098230362 faiss_hnsw faiss ({‘M’: 36, ‘efConstruction’: 500}, ef: 80) 888936.0 1585.1033165454865 faiss_hnsw faiss ({‘M’: 48, ‘efConstruction’: 500}, ef: 80) 982148.0 1667.5446255207062 说明1.efConstruction越小，build 耗费时间越小，牺牲的精确性可以通过加大ef来弥补, 要想获取最佳性能需要对M，efConstruction, ef这三个参数进行平衡2.indexsize 只和M有关系3.整体而言，hnswlib的性能仍然要比faiss的hnsw的要好一点, 两者差距不大4.efConstruction&#x3D;200的情况下通过适当调大ef也能实现较高的召回率，也不会带来性能损失，但是对Build Index Time 会有较大的提升 其他细节指标解读 ann-benchmarks中通过python data_export.py可以得出详细的指标数据, 其中有两列epsilon, largeepsilon这里其实对应得是在算recall的时候，允许的距离误差大小(使用euclidean距离，就是向量召回的点和实际的knn的点距离误差), epsilon 是0.01， largeepsilon, 允许的误差越大，对应的recall 就越高, 论文中的实际公式如下: indexsize在graph base的算法中都挺大的，不过整体上hnswlib的更小一点 总结 这里列举都是在特定数据集下的表现，事实上不同数据集下不同类型的算法表现不尽相同，hnswlib不是在任何情况下的表现都由于faiss_hnsw， 那在实际的生产环境中，还是需要对不同的算法进行benchmark，从而得出更好的参数和选出更好的算法 ann-benchmarks的论文指出虽然graph-based的算法在rand-euclidean的数据集下性能还不如faiss-ivf，但是在实际的数据集上，往往会有所谓的global structure出现，graph based的算法一般都是更好的选择，graph based中的hnsw也是更好的选择 ann-benchmarks也指出如果使用GPU的话，graph based的算法不如ivf这种简单的算法","categories":[],"tags":[{"name":"algorithm hnsw 向量数据库 faiss hnswlib","slug":"algorithm-hnsw-向量数据库-faiss-hnswlib","permalink":"https://skyitachi.github.io/tags/algorithm-hnsw-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-faiss-hnswlib/"}]},{"title":"HNSW算法原理及其实现","slug":"HNSW算法原理及其实现","date":"2024-07-21T16:00:00.000Z","updated":"2024-07-21T16:00:00.000Z","comments":true,"path":"2024/07/22/HNSW算法原理及其实现/","permalink":"https://skyitachi.github.io/2024/07/22/HNSW%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"主要内容 介绍HNSW的算法原理 介绍hnswlib和faiss中的实现，以及他们之间的区别 介绍HNSW中每个参数的实际影响 HNSW的算法原理概述 HNSW的算法大致原理很像skplist，区别在于hnsw每层维护的是节点直接的链接（links）,每一层都是graph的结构，相同点就是每下一层的节点数更多,维护的图也越大，到了0层维护了所有节点的链接信息 无论是插入还是查找，都是从上往下找enter point（当前层和插入或者查询点最接近的点）,到了下一层时候，通过在enter point的链接的点中早出指定数量的候选集，并选出和目标点最接近的指定数量的点，建立连接 插入算法还需要维护当前的enter point 具体算法论文中有提到一些最佳参数设置，mL &#x3D; 1&#x2F;ln(M), Mmax &#x3D; M, Mmax0 &#x3D; M * 2 可以直接带入算法中的参数，减少未知量，看起来会清晰一点，其中hnswlib的实现就是按照这组参数来的 SEARCH-LAYER算法SEARCH-LAYER(q, ep, ef, lc) Input: query element q, enter points ep, number of nearest to q elements to return ef, layer number lc Output: ef closest neighbors to q 123456789101112131415161718192021v ← ep // set of visited elementsC ← ep // set of candidatesW ← ep // dynamic list of found nearest neighborswhile │C│ &gt; 0 c ← extract nearest element from C to q f ← get furthest element from W to q if distance(c, q) &gt; distance(f, q) break // all elements in W are evaluated for each e ∈ neighbourhood(c) at layer lc // update C and W if e ∉ v v ← v ⋃ e f ← get furthest element from W to q if distance(e, q) &lt; distance(f, q) or │W│ &lt; ef C ← C ⋃ e W ← W ⋃ e if │W│ &gt; ef remove furthest element from W to qreturn W 说明 ep可以是由多个点组成的集合，也可以是单个点 插入算法INSERT(hnsw, q, M, Mmax, efConstruction, mL) Input: multilayer graph hnsw, new element q, number of established connections M, maximum number of connections for each element per layer Mmax, size of the dynamic candidate list efConstruction, normalization factor for level generation mL Output: update hnsw inserting element q 1234567891011121314151617181920212223W ← ∅ // list for the currently found nearest elementsep ← get enter point for hnswL ← level of ep // top layer for hnswl ← ⌊-ln(unif(0..1))∙mL⌋ // new element’s levelfor lc ← L … l+1 W ← SEARCH-LAYER(q, ep, ef=1, lc) ep ← get the nearest element from W to qfor lc ← min(L, l) … 0 W ← SEARCH-LAYER(q, ep, efConstruction, lc) neighbors ← SELECT-NEIGHBORS(q, W, M, lc) // alg. 3 or alg. 4 add bidirectionall connectionts from neighbors to q at layer lc for each e ∈ neighbors // shrink connections if needed eConn ← neighbourhood(e) at layer lc if │eConn│ &gt; Mmax // shrink connections of e if lc = 0 then Mmax = Mmax0 eNewConn ← SELECT-NEIGHBORS(e, eConn, Mmax, lc) // alg. 3 or alg. 4 set neighbourhood(e) at layer lc to eNewConn ep ← Wif l &gt; L set enter point for hnsw to q entry point的更新 新插入的节点最终会变为全局的enter pointer level0会维护2*M的链接 搜索算法K-NN-SEARCH(hnsw, q, K, ef) Input: multilayer graph hnsw, query element q, number of nearest neighbors to return K, size of the dynamic candidate list ef Output: K nearest elements to q 123456789W ← ∅ // set for the current nearest elementsep ← get enter point for hnswL ← level of ep // top layer for hnswfor lc ← L … 1 W ← SEARCH-LAYER(q, ep, ef=1, lc) ep ← get nearest element from W to qW ← SEARCH-LAYER(q, ep, ef, lc =0)return K nearest elements from W to q 说明 搜索过程从L-&gt;1 层每层只搜和query最近的enter point, 这就要求数据集是要有一定结构的，这样才能保证在最后一层搜索的时候质量不至于太差，如果是随机的数据集可能结果会不太好 efSearch要大于K才行(faiss里会保证efSearch最少是k) SELECT-NEIGHBORS 算法简单算法123456SELECT-NEIGHBORS-SIMPLE(q, C, M)Input: base element q, candidate elements C, number of neighbors to return MOutput: M nearest elements to qreturn M nearest elements from C to q// 实际的实现就是一个优先权队列 启发式方法SELECT-NEIGHBORS-HEURISTIC(q, C, M, lc, extendCandidates, keepPrunedConnections) Input: base element q, candidate elements C, number of neighbors to return M, layer number lc, flag indicating whether or not to extendcandidate list extendCandidates, flag indicating whether or not to add discarded elements keepPrunedConnections Output: M elements selected by the heuristic 12345678910111213141516171819202122R ← ∅W ← C // working queue for the candidatesif extendCandidates // extend candidates by their neighbors for each e ∈ C for each eadj ∈ neighbourhood(e) at layer lc if eadj ∉ W W ← W ⋃ eadjWd ← ∅ // queue for the discarded candidateswhile │W│ &gt; 0 and │R│&lt; M e ← extract nearest element from W to q if e is closer to q compared to any element from R R ← R ⋃ e else Wd ← Wd ⋃ eif keepPrunedConnections // add some of the discarded // connections from Wd while │Wd│&gt; 0 and │R│&lt; M R ← R ⋃ extract nearest element from Wd to qreturn R 说明 第一种方法只从candidates选择，第二种方法从candidates里的邻接的点里选择（选择范围更大, 可能受数据集的影响较小） hnswlib和faiss_hnsw都相当于用了第一种方法 启发式方法适合中维数据和多clustering的数据( mid-dimensional data and for the case of highly clustered data) HNSW每个参数实际的影响 dim: 数据维度，dim 越大计算量越大 max_elements: 数据总量 M: 每个向量的最大链接数, M越大占用的内存越大, 构建和搜索也是越慢（精度更高） efConstruction: 每个向量构建或者搜索时的候选集大小，efConstruction越大，构建和搜索速度越慢 (不会影响内存消耗) M 应该如何选择 论文中指出A reasonable range of M is from 5 to 48 efConstruction efConstruction 对speed&#x2F;index quality有显著影响 论文中指出10M的sift dataset，用efConstruction&#x3D;100就能达到不错的召回率(0.95), 适用多线程并发构建的话速度也不错 进一步提升efConstruction带来的收益并不明显，反而会较大影响构建速度 efSearch 搜索时候用到ef值，hnswlib和faiss_hnsw都可以单独设置 合适的efSearch能保证recall efSearch也不是越高越好，边际效应越来越小，也会影响到搜索速度 总结 关于详细的性能数据可以参考论文 测试性能的时候关键指标是 Distance Computations, Query Time 可以使用PQ（乘积量化）的方式优化memory","categories":[],"tags":[{"name":"algorithm hnsw 向量数据库","slug":"algorithm-hnsw-向量数据库","permalink":"https://skyitachi.github.io/tags/algorithm-hnsw-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"ES和Lucene之间的CRUD操作映射","slug":"ES和Lucene之间的CRUD操作映射","date":"2024-05-10T16:00:00.000Z","updated":"2024-07-06T16:00:00.000Z","comments":true,"path":"2024/05/11/ES和Lucene之间的CRUD操作映射/","permalink":"https://skyitachi.github.io/2024/05/11/ES%E5%92%8CLucene%E4%B9%8B%E9%97%B4%E7%9A%84CRUD%E6%93%8D%E4%BD%9C%E6%98%A0%E5%B0%84/","excerpt":"","text":"前言在ES中我们经常使用的数据格式json，ES也支持常见的CRUD操作，这里我们主要介绍写入相关的操作（创建，更新，删除），ES的底层存储引擎是Lucene，Lucene也有相关创建更新删除的操作，但是Lucene是没有显示的根据主键更新文档的api的，本文主要介绍的是在ES有_id的情况下，ES是如何基于Lucene实现增删改的操作的，其中的数据模型又是如何映射的. ps: 本文不考虑ES中的数据类型到Lucene中的数据类型的映射（Field），所有的代码片段都是基于以下给定的类型映射 Name ES type Lucene Field item_id keyword StringField name keyword StringField color keyword StringField lucene 如何基于id（主键）部分更新文档 (基于lucene 9.7.0)12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;9.7.0&lt;/version&gt;&lt;/dependency&gt; 1. 创建文档123456789101112131415161718public static void createDocuments(IndexWriter indexWriter) throws IOException &#123; Document doc1 = new Document(); doc1.add(new StringField(&quot;item_id&quot;, &quot;1&quot;, Field.Store.YES )); doc1.add(new StringField(&quot;name&quot;, &quot;item1&quot;, Field.Store.YES)); doc1.add(new StringField(&quot;color&quot;, &quot;red&quot;, Field.Store.YES)); Document doc2 = new Document(); doc2.add(new StringField(&quot;item_id&quot;, &quot;2&quot;, Field.Store.YES)); doc2.add(new StringField(&quot;name&quot;, &quot;item2&quot;, Field.Store.YES)); doc2.add(new StringField(&quot;color&quot;, &quot;blue&quot;, Field.Store.YES)); indexWriter.addDocument(doc1); indexWriter.addDocument(doc2); indexWriter.commit(); indexWriter.flush();&#125; 2. 读取文档验证写入1234567891011121314public static void getDocumentById(IndexReader reader, IndexSearcher searcher, String itemId) throws IOException &#123; Query query = new TermQuery(new Term(&quot;item_id&quot;, itemId)); TopDocs topdocs = searcher.search(query, 10); assert topdocs.totalHits.value == 1; for (ScoreDoc doc: topdocs.scoreDocs) &#123; int docId = doc.doc; Document fullDoc = getDocumentByDocId(reader, docId); fullDoc.getFields().forEach(field -&gt; &#123; System.out.println(&quot; &quot; + field.name() + &quot;: &quot; + field.stringValue()); &#125;); &#125;&#125; 123456789// item_id = &quot;1&quot;getDocumentById(reader, searcher, &quot;1&quot;);// item_id: 1// name: item1// color: redgetDocumentById(reader, searcher, &quot;2&quot;);// item_id: 2// name: item2// color: blue 3. 部分更新文档123456789public static void partialUpdateDocumentNameByItemId(IndexWriter writer, String itemId, String newName) throws IOException &#123; Document doc = new Document(); doc.add(new StringField(&quot;name&quot;, newName, Field.Store.YES)); // important: 这个作为主键的Term一定要带上 doc.add(new StringField(&quot;item_id&quot;, itemId, Field.Store.YES)); Term mainTerm = new Term(&quot;item_id&quot;, itemId); writer.updateDocument(mainTerm, doc);&#125; 4. 验证更新文档123456789101112131415161718partialUpdateDocumentNameByItemId(indexWriter, &quot;1&quot;, &quot;item1_updated&quot;);partialUpdateDocumentNameByItemId(indexWriter, &quot;2&quot;, &quot;item2_updated&quot;);indexWriter.commit();indexWriter.flush();indexReader = DirectoryReader.open(readDirectory);indexSearcher = new IndexSearcher(indexReader);getDocumentById(indexSearcher, &quot;1&quot;);indexReader = DirectoryReader.open(readDirectory);indexSearcher = new IndexSearcher(indexReader);getDocumentById(indexSearcher, &quot;2&quot;);//// name: item1_updated// item_id: 1// name: item2_updated// item_id: 2 结论： 由于在partialUpdateDocumentNameByItemId 中只写了item_id和name属性（符合update的直觉），但是可以看出Lucene把updateDocument中的doc对象当成了最新且完整的mainTerm对应的doc，这就导致了虽然我们目的是部分更新，但是会丢失没有写入（没有变化）的那些属性，这个例子也可以看出Lucene本质上是用新文档覆盖旧文档的形式，用一个可以代表主键的Term做关联，来实现部分更新字段的目的，这个和一般RDBMS的存储模型有点区别. 5. 完全更新所有字段（不变的field也要加入将要更新的document中）123456789public static void fullUpdateDocumentNameByItemId(IndexWriter writer, String itemId, String newName, String oldColor) throws IOException &#123; Document doc = new Document(); doc.add(new StringField(&quot;name&quot;, newName, Field.Store.YES)); doc.add(new StringField(&quot;item_id&quot;, itemId, Field.Store.YES)); doc.add(new StringField(&quot;color&quot;, oldColor, Field.Store.YES)); Term mainTerm = new Term(&quot;item_id&quot;, itemId); writer.updateDocument(mainTerm, doc);&#125; 6. 验证完全更新123456789101112131415161718192021fullUpdateDocumentNameByItemId(indexWriter, &quot;1&quot;, &quot;item1_updated&quot;, &quot;red&quot;);fullUpdateDocumentNameByItemId(indexWriter, &quot;2&quot;, &quot;item2_updated&quot;, &quot;blue&quot;);indexWriter.commit();indexWriter.flush();indexReader = DirectoryReader.open(readDirectory);indexSearcher = new IndexSearcher(indexReader);getDocumentById(indexSearcher, &quot;1&quot;);indexReader = DirectoryReader.open(readDirectory);indexSearcher = new IndexSearcher(indexReader);getDocumentById(indexSearcher, &quot;2&quot;);// output// name: item1_updated// item_id: 1// color: red// name: item2_updated// item_id: 2// color: blue 7. 删除文档1234public static void deleteDocument(IndexWriter writer, String itemId) throws IOException &#123; Term mainTerm = new Term(&quot;item_id&quot;, itemId); writer.deleteDocuments(mainTerm);&#125; 8. 验证删除文档12345getDocumentById(indexSearcher, &quot;1&quot;);getDocumentById(indexSearcher, &quot;2&quot;);// cannot found 1// cannot found 2 结论： 必须将原始文档的所有字段全部获取到再用updateDocument的方式更新，才能实现我们预期中部分更新字段的目的，可以看到成本还是比较高的 由于Lucene的这种机制也导致了，ES的CRUD模型中需要实现一些额外的机制才能使用到Lucene的能力 ES中的操作ES mapping12345678910111213141516PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;item_id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;color&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125; 1. 创建文档12345678910111213PUT my_index/_doc/1&#123; &quot;item_id&quot;: &quot;1&quot;, &quot;name&quot;:&quot;item1&quot;, &quot;color&quot;: &quot;red&quot;&#125;PUT my_index/_doc/2&#123; &quot;item_id&quot;: &quot;2&quot;, &quot;name&quot;:&quot;item2&quot;, &quot;color&quot;: &quot;blue&quot;&#125; 2. 获取文档123456789101112131415161718192021222324252627GET my_index/_search// output&#123; ... &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;item_id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;item1&quot;, &quot;color&quot;: &quot;red&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;item_id&quot;: &quot;2&quot;, &quot;name&quot;: &quot;item2&quot;, &quot;color&quot;: &quot;blue&quot; &#125; &#125; ]&#125; 3.部分更新12345678910111213POST my_index/_update/1&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;item1_updated&quot; &#125;&#125;POST my_index/_update/2&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;item2_updated&quot; &#125;&#125; 4.获取部分更新后的文档123456789101112131415161718192021222324252627GET my_index/_search// output&#123; ... &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;item_id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;item1_updated&quot;, &quot;color&quot;: &quot;red&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;item_id&quot;: &quot;2&quot;, &quot;name&quot;: &quot;item2_updated&quot;, &quot;color&quot;: &quot;blue&quot; &#125; &#125; ]&#125; 说明: 可以看ES中部分更新是可以正常的工作的, 原因就在于ES在处理update的时候会自动拉取原始文档的所有字段和新的更新的字段组合成一份完整的新的全量字段的文档，再去更新Lucene 5. 完整更新所有字段1234567891011121314151617// 方式一POST my_index/_update/1&#123; &quot;doc&quot;: &#123; &quot;name&quot;: &quot;item1_updated&quot;, &quot;item_id&quot;: &quot;1&quot;, &quot;color&quot;: &quot;red&quot; &#125;&#125;// 方式二PUT my_index/_doc/1&#123; &quot;item_id&quot;: &quot;1&quot;, &quot;name&quot;:&quot;item1_update_by_put&quot;, &quot;color&quot;: &quot;red&quot;&#125; 6. 验证完全更新123456789101112131415161718192021222324252627GET my_index/_search// output&#123; ... &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;item_id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;item1_update_by_put&quot;, &quot;color&quot;: &quot;red&quot; &#125; &#125;, &#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_id&quot;: &quot;2&quot;, &quot;_score&quot;: 1, &quot;_source&quot;: &#123; &quot;item_id&quot;: &quot;2&quot;, &quot;name&quot;: &quot;item2_updated&quot;, &quot;color&quot;: &quot;blue&quot; &#125; &#125; ]&#125; 7. 删除文档12DELETE my_index/_doc/1DELETE my_index/_doc/2 8. 验证删除文档12345678GET my_index/_search// output&#123; ... &quot;hits&quot;: []&#125; ES如何在Lucene的基础上实现基于主键(_id)的删改 ES 使用_id的内部字段作为文档的主键，这个_id可以用户指定，上面的例子中item_id就是对应了_id，Lucene中没有主键的概念，所以需要使用_id 作为一个独特的Term的维持文档的唯一性, 后续的更新, 删除也是和_id对应的Term绑定. Lucene的创建操作不具备幂等性（addDocument）指定了Term之后，这个Term下可以关联N个document，不具备唯一性. ES 在部分字段的更新中，自己封装了一层获取原始文档的操作，之后使用update的方式更新Lucene. ES 在删除文档操作中，使用_id对应的Term 去调用Lucene的API. ES 在创建文档操作中，PUT 相同_id的文档 同样能保持唯一性, 通常情况下ES也是用Lucene update的方式实现创建的请求. 总结 Lucene整体是一个Append Only的存储引擎，且没有主键的概念. ES 本身封装了一系列的操作使得整个CRUD操作更加方便使用，这也不可避免的带了一些额外的开销，通过理解这些操作的底层原理，有助于我们做出一些最佳实践的选择.","categories":[],"tags":[{"name":"ES, Lucene, elasticsearch","slug":"ES-Lucene-elasticsearch","permalink":"https://skyitachi.github.io/tags/ES-Lucene-elasticsearch/"}]},{"title":"Golang 中[]byte, string和[]rune的相互转化的底层原理和剖析","slug":"golang_slice","date":"2021-02-21T02:15:38.000Z","updated":"2021-02-21T02:15:38.000Z","comments":true,"path":"2021/02/21/golang_slice/","permalink":"https://skyitachi.github.io/2021/02/21/golang_slice/","excerpt":"","text":"Golang 中[]byte, string和[]rune的相互转化的底层原理和剖析在golang中有些场景经常会用到[]byte和string的相互转化，尤其是在使用json.Marshal和json.Unmarshal的时候，经常会遇到需要这种转化。 本文主要说明以下内容： 几种类型相互转化的方法和性能分析 这些类型的底层存储 代码gist 相互转化[]byte和string的相互转化string -&gt; []byte1234567891011121314151617181920212223242526func BenchmarkStringToByteSlice(b *testing.B) &#123; s := genString(10000) b.ReportAllocs() for i := 0; i &lt; b.N; i++ &#123; bs := []byte(s) if len(bs) != len(s) &#123; b.Error(&quot;error&quot;) &#125; &#125;&#125;func BenchmarkStringToByteSliceUnsafe(b *testing.B) &#123; s := genString(10000) b.ReportAllocs() for i := 0; i &lt; b.N; i++ &#123; l := len(s) bs := *(*[]byte)(unsafe.Pointer(&amp;reflect.SliceHeader&#123; Data: (*(*reflect.StringHeader)(unsafe.Pointer(&amp;s))).Data, Len: l, Cap: l, &#125;)) if len(bs) != len(s) &#123; b.Error(&quot;error&quot;) &#125; &#125;&#125; 第一种使用[]byte这种直接转化，也是我们常用的方式，第二种是使用unsafe的方式。这两种区别就在于一个是重新分配了内存，另一个是复用了原来的内存。 benchmark的结果也验证了这一点 12345678910go test -run=BenchmarkStringToByteSlice -bench=StringToByteSlice# go-demo.testgoos: darwingoarch: amd64pkg: go-demoBenchmarkStringToByteSlice-12 1164224 964 ns/op 10285 B/op 1 allocs/opBenchmarkStringToByteSliceUnsafe-12 1000000000 0.380 ns/op 0 B/op 0 allocs/opPASSok go-demo 2.089s []byte -&gt; string12345678910111213141516171819202122func BenchmarkSliceByteToString(b *testing.B) &#123; bs := genSliceByte(100) b.ReportAllocs() for i := 0; i &lt; b.N; i++ &#123; s := string(bs) if len(s) != len(bs) &#123; b.Error(&quot;error&quot;) &#125; &#125;&#125;func BenchmarkSliceByteToStringUnsafe(b *testing.B) &#123; bs := genSliceByte(100) b.ReportAllocs() for i := 0; i &lt; b.N; i++ &#123; s := *(*string)(unsafe.Pointer(&amp;bs)) if len(s) != len(bs) &#123; b.Log(&quot;slice: &quot;, len(bs), &quot; string: &quot;, len(s)) b.Error(&quot;error: &quot;) &#125; &#125;&#125; benchmark 结果 12345678910go test -run=BenchmarkSliceByteToString -bench=SliceByteToString# go-demo.testgoos: darwingoarch: amd64pkg: go-demoBenchmarkSliceByteToString-12 35913873 32.4 ns/op 112 B/op 1 allocs/opBenchmarkSliceByteToStringUnsafe-12 1000000000 0.253 ns/op 0 B/op 0 allocs/opPASSok go-demo 3.796s string和[]rune的相互转化string和rune的相互转化其实和上面类似，主要是[]rune对应的[]byte数组长度需要计算下，这里就只贴一个[]rune到string的转化了 123456789101112131415161718func BenchmarkSliceRuneToStringUnsafe(b *testing.B) &#123; bs := genSliceRune(100) s1 := string(bs) b.ReportAllocs() for i := 0; i &lt; b.N; i++ &#123; var l int for _, r := range bs &#123; l += utf8.RuneLen(r) &#125; s := *(*string)(unsafe.Pointer(&amp;reflect.StringHeader&#123; Data: (*(*reflect.SliceHeader)(unsafe.Pointer(&amp;bs))).Data, Len: l, &#125;)) if len(s1) != len(s) &#123; b.Error(&quot;error&quot;) &#125; &#125;&#125; String和Slice的底层存储分析reflect.SliceHeader 和reflect.StringHeader123456789type StringHeader struct &#123; Data uintptr Len int&#125;type SliceHeader struct &#123; Data uintptr Len int Cap int&#125; 两者类型基本一样，Slice多了一个Cap，其实这也决定了[]byte可以直接使用指针强转成string，但是反过来却不行 slice的底层存储12345type slice struct &#123; array unsafe.Pointer len int cap int&#125; 以汇编的形式看下slice的底层结构1234package pkg// var data = make([]int, 0, 10)var data = []int&#123;1, 2&#125; 12345678910go tool compile -S pkg.gogo.cuinfo.packagename. SDWARFINFO dupok size=0 0x0000 70 6b 67 pkg&quot;&quot;.data SDATA size=24 0x0000 00 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 ................ 0x0010 02 00 00 00 00 00 00 00 ........ rel 0+8 t=1 &quot;&quot;..stmp_0+0&quot;&quot;..stmp_0 SNOPTRDATA size=16 0x0000 01 00 00 00 00 00 00 00 02 00 00 00 00 00 00 00 ................... 可以看到””.data 对应的是size是24（8byte的指针，len和cap各自8byte），slice里的内容是两个int对应的就是，””.stmp_0 里的内容 进一步分析data对应的二进制 data+8是02 00 ... ，对应len data+16是02 00 对应cap 整个slice struct在内存里是紧凑分布的，所以我们可以进行指针类的强制转化，类似于c++中reinterpret_cast string的底层结构1234package pkgvar testStr = &quot;abc&quot; 1234567go.cuinfo.packagename. SDWARFINFO dupok size=0 0x0000 70 6b 67 pkggo.string.&quot;abc&quot; SRODATA dupok size=3 0x0000 61 62 63 abc&quot;&quot;.testStr SDATA size=16 0x0000 00 00 00 00 00 00 00 00 03 00 00 00 00 00 00 00 ................ rel 0+8 t=1 go.string.&quot;abc&quot;+0 和上文的slice很类似，size变成了16而已 Fat Pointer像slice这种结构在c中常被称为fatpointer，感兴趣的同学可以参考Go Slices are Fat Pointers 总结 介绍了golang中string，[]byte和[]rune的转化及简单的性能分析 slice在golang中的底层存储","categories":[],"tags":[{"name":"golang slice","slug":"golang-slice","permalink":"https://skyitachi.github.io/tags/golang-slice/"}]},{"title":"二分法的两种实现","slug":"二分法的两种实现","date":"2020-04-14T14:15:38.000Z","updated":"2020-04-14T14:11:39.000Z","comments":true,"path":"2020/04/14/二分法的两种实现/","permalink":"https://skyitachi.github.io/2020/04/14/%E4%BA%8C%E5%88%86%E6%B3%95%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"前言虽然二分搜索很简单（在无重复的有序数组上）,但是也有很多值得注意的地方，而且有两种完全不同的写法（两种完全不同的功能） lower bound 找出大于等于target的最小数组下标, 不存在的情况下返回-1 123456789101112131415161718func lower_bound(a []int, target int) int &#123; l := 0 h := len(a) - 1 for l &lt; h &#123; m := l + (h - l) / 2 if a[m] &gt;= target &#123; h = m &#125; else &#123; // l肯定可以取到h值，所以不需要使用向上取整计算m值 l += 1 &#125; &#125; if a[l] &gt;= target &#123; return l &#125; return -1&#125; upper bound 找出小于等于target的最大数组下标 1234567891011121314151617func upper_bound(a []int, target int) int &#123; l := 0 h := len(a) - 1 for l &lt; h &#123; m := l + (h - l + 1) / 2 if a[m] &lt;= target &#123; // l 要能够取到h值，就必须保证m使用向上取整计算, (h - l + 1) / 2 就是这么来的 l = m &#125; else &#123; h = m - 1 &#125; &#125; if a[l] &lt;= target &#123; return l &#125; return -1&#125; 注意点 计算mid的时候不能发生溢出 数组下标不能越界","categories":[],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://skyitachi.github.io/tags/algorithm/"}]},{"title":"dubbo-go support protobuf","slug":"dubbo-go-protobuf-support","date":"2019-12-29T16:00:00.000Z","updated":"2019-12-29T16:00:00.000Z","comments":true,"path":"2019/12/30/dubbo-go-protobuf-support/","permalink":"https://skyitachi.github.io/2019/12/30/dubbo-go-protobuf-support/","excerpt":"","text":"主要用法 和grpc中使用protobuf生成代码基本一致（至少在形式上）,直接看例子吧 123456789├── go-client│ ├── client.go│ ├── client.yml├── go-server│ ├── main.go│ └── server.yml└── user ├── user.pb.go └── user.proto 1234567891011121314151617syntax = &quot;proto3&quot;;package user;service UserProvider &#123; rpc GetUser (UserRequest) returns (UserReply) &#123;&#125;&#125;message UserRequest &#123; string id = 1;&#125;message UserReply &#123; string id = 1; string name = 2; int32 age = 3;&#125; 使用protoc-gen-dubbogo插件生成dubbogo的代理 1protoc --plugin=&#123;plugin_path&#125; --dubbogo_out=plugins=dubbogo:. user/user.proto client关键代码 1234567user := user.NewUserProvider()reply := user.UserReply&#123;&#125;err := userProvider.GetUser(context.TODO(), &amp;user.UserRequest&#123;Id: &quot;A001&quot;&#125;, &amp;reply)if err != nil &#123; log.Fatal(err)&#125;println(&quot;response result: %+v&quot;, reply) ps: 一切都是熟悉的味道ps: 生成代理的名称需要和reference里配置的一样 server关键代码 12345678910type UserProvider struct &#123; pb.UnimplementedUserProviderServer&#125;func (*UserProvider) GetUser(ctx context.Context, user *pb.UserRequest) (*pb.UserReply, error) &#123; return &amp;pb.UserReply&#123;Id: &quot;001&quot;, Name: &quot;alice&quot;, Age: 18&#125;, nil&#125;// 注册pb.RegisterProvider(new(UserProvider)) 实现原理 在dubbogo抽出一层serialization，任何和serialization相关的之后只要实现Serialize接口就行了，这样是为了更好的实现更多序列化的支持，逻辑上会更合理一些，原有的go hessian2中做了一部分dubbo相关的codec工作，这里我也把它抽到dubbogo中了， 当然hessian2的序列化仍然保留了，这次实现是兼容老版本的。 12345type Serializer interface &#123; Marshal(p DubboPackage) ([]byte, error) Unmarshal([]byte, *DubboPackage) error&#125; 参考了dubbo的protobuf实现，实现了在protobuf层面和java互通（不一定是好事:(） 其他的就是细节了 一些注意点 error的处理和java不太一样，java会把详细的java error stack都返回给客户端，go只会把message传过来，生成一个error 由于java protobuf生成的代理方法名是小写开头(完全搞不明白是为什么)，这在golang中表示私有方法，个人已经提了issue, 所以直接用java的例子是不行的 java protobuf的代理生成的是内部接口，比如xxx$IDemoService, $是url中的一个特殊字符，正好发现了dubbogo的一个注册url的bug 我为什么要支持protobuf protobuf的语言中立性更好，序列化性能也更好 更加符合golang的生态","categories":[],"tags":[{"name":"dubbo, go, protobuf","slug":"dubbo-go-protobuf","permalink":"https://skyitachi.github.io/tags/dubbo-go-protobuf/"}]},{"title":"c++中的传参","slug":"passing-value-in-cpp","date":"2019-08-21T02:30:36.000Z","updated":"2019-08-21T02:30:36.000Z","comments":true,"path":"2019/08/21/passing-value-in-cpp/","permalink":"https://skyitachi.github.io/2019/08/21/passing-value-in-cpp/","excerpt":"","text":"前言最近在用c++写基于libuv的websocket engine的时候发现, 设置callback的参数是一个很有挑战性的工作, 原来觉得c++的复杂在于其模板，oo范式概念的复杂, 现在发现c++的每个方面都很复杂，因为有太多可以通过编译的方式了，我想从传参这个方面切入，让大家了解下c++的复杂（强大）。 ps：本文的传参使基于涉及到动态内存分配对象的传参，一般普通对象的传参基本是不需要考虑这么复杂的(至少我目前这么认为)。 以下是本文中需要传递的参数，一个简单的String, 只保留会讲到的构造函数 1234567891011121314151617class String: &#123; public: String(const char* src): data_(new char[strlen(src) + 1]), size_(strlen(src)) &#123; ::strcpy(data_, src); data_[size_] = 0; &#125; String(const String&amp; lhs): data_(new char[lhs.size() + 1]), size_(lhs.size()) &#123; ::strcpy(data_, lhs.data()); data_[size_] = 0; &#125; // move String(String &amp;&amp;rhs) noexcept: data_(rhs.data_), size_(rhs.size_) &#123; rhs.data_ = nullptr; &#125; &#125;; 我遇到的一个问题是在MessageCallback中，应该使用const String&amp; message还是String&amp;&amp; message, 这两种形参的区别是什么 理解std::move 和右值引用在弄清上述问题之前，还是要从根本上着手，弄清std::move和右值引用。右值引用是c++11中引入的一种新的引用类型，必须要绑定到右值的引用。而std::move的作用是可以把几乎任意值转化成一个右值引用。 123456789101112131415161718void test_passing_value(std::string&amp; s1) &#123; std::cout &lt;&lt; &quot;in the left reference&quot; &lt;&lt; std::endl;&#125;void test_passing_value(std::string&amp;&amp; s1) &#123; std::cout &lt;&lt; &quot;in the right reference&quot; &lt;&lt; std::endl;&#125;std::string s1 = &quot;hello&quot;;std::string &amp;sr = s1;test_passing_value(s1); // in the lefttest_passing_value(std::move(s1)); // in the righttest_passing_value(sr); // in the lefttest_passing_value(std::move(sr)); // in the right// 可以看到无论是左值，还是左值引用，使用std::move之后都可以变成右值引用// 意外的情况就是const T&amp; 在使用std::move转化时的特殊情况 const T&amp; vs T&amp;&amp;c++11中引入了右值引用和move语义，初学者（比如我）很容易被这种特性吸引（move比copy快）, 两者其实是解决不同场景下的问题，T&amp;&amp; 的确提供了一种更为高效的传参方式, 让我们看下两者的细节和使用场景吧。 仅仅使用const T&amp;并不会发生copy 12345678void foo(const String&amp; s) &#123; std::cout &lt;&lt; s &lt;&lt; std::endl;&#125;int main() &#123; String s0(&quot;hello&quot;); foo(s0); // 不会发生复制&#125; const T&amp; 发生复制的情况是在函数体内用到T的local variable， 比如T local = t, 这时候会发生拷贝控制 仅仅使用T&amp;&amp;不会发生move 1234567void f2(String &amp;&amp;s2) &#123; std::cout &lt;&lt; s2 &lt;&lt; std::endl;&#125;int main() &#123; f2(String(&quot;hello&quot;));&#125; 从以上两种情况来看似乎传参的代价都很低，那么应该如何选择呢，主要还是根据语义来做选择，如果你的实参是个左值自然选择第一种，如果是右值那自然是后者，如果你确定需要第二种那么使用std::move也是可以的 结论 由于我在传给callback的string是从buffer中复制构造来的，而不是仅仅像stringpiece那样使用，所以使用右值引用更合适，使用者会放心大胆的使用这个string，move之类的更不在话下了 其他 可以考虑使用StringPiece类似的技术，不过我感觉StringPiece在这个场景下并不好 关于std::move的原理其中涉及到了引用折叠这些比较复杂的概念，所以没有深入介绍 在模板中使用T&amp;&amp; 和实参中的&amp;&amp;还是不一样的，模板中的T&amp;&amp; 在转发参数时要保证不丢失T的信息(T可能是引用) 所以有涉及到了完美转发的概念，std::forward可以解决这个问题","categories":[],"tags":[{"name":"cpp","slug":"cpp","permalink":"https://skyitachi.github.io/tags/cpp/"}]},{"title":"浅谈 javascript 作用域","slug":"scope","date":"2017-04-23T16:00:00.000Z","updated":"2017-04-23T16:00:00.000Z","comments":true,"path":"2017/04/24/scope/","permalink":"https://skyitachi.github.io/2017/04/24/scope/","excerpt":"","text":"前言 本文将主要介绍javascript中作用域相关的问题，尽可能多的使用代码举例说明，尽量少涉及动态作用域相关 词法作用域(核心) javascript的作用域是词法作用域（静态作用域）, 不过像eval，with这些具有动态改变作用域的能力, 本文重点在于词法作用域 1234567891011121314151617181920212223242526272829303132333435363738let a = 1;function testLexicalScope() &#123; console.log(a); // 由于当前scope中a是由最外层定义的，所以此处的a只能访问到最外层的a&#125;function b() &#123; let a = 2; testLexicalScope();&#125;b();// 另一个例子let sameVar1 = 1;let sameVar2 = 1;function outerScope() &#123; let sameVar1 = 2; function innerScope() &#123; console.log(&quot;current scope: sameVar1 is &quot;, sameVar1); // 当前的scope中最近的sameVar1值是2 console.log(&quot;current scope: sameVar2 is &quot;, sameVar2); &#125; innerScope();&#125;outerScope();// current scope: sameVar1 is 2// current scope: sameVar2 is 1//另一个例子const f1 = function () &#123; console.log(outVar);&#125;;let outVar = 1;f1(); // 1 why 只有在函数声明的时候才会遵循lexical scope的规则, 如果是函数表达式则取决于调用的时机 不同类型的作用域(如何创建scope) 函数作用域 属于这个函数的全部变量都可以在整个函数的范围内使用及复用javascript 每个函数都会创建一个scope 12345678function fScope() &#123; var aStr = &quot;function&quot;; console.log(aStr);&#125;fScope();console.log(aStr); // ReferenceError: aStr is not defined 块级作用域({…}) 12345678var foo = true;&#123; // 这里是使用let，将foo绑定到了&#123;&#125;这个块作用域中 let foo = false; let bar = &quot;cannot seen&quot;; console.log(foo); // false&#125;console.log(foo); // trueconsole.log(bar); // ReferenceError var 函数作用域中的var仍然遵循函数作用域相关的 var中没有块级作用域 1234567var foo = true;&#123; var foo = false; var bar = true;&#125;console.log(foo); // falseconsole.log(bar); // bar 变量提升 变量和函数的所有声明多会在任何代码被执行前首先被处理（编译器找到这些变量与合适的作用域关联） 1234567function hoisting() &#123; console.log(a); var a = 1; console.log(a);&#125;hoisting(); 又是let 123456function hoisting() &#123; console.log(a); // 第一个let之上的区域叫做`temporal dead zone` let a = 1; console.log(a);&#125;hoisting(); // ReferenceError 函数声明的优先级会高于变量声明 1234567foo(); // in the functionvar foo = 1;function foo() &#123; console.log(&quot;in the function&quot;);&#125; 总结 关于块级作用域, 可用try{} catch(err) {&#x2F;这里是块级作用域&#x2F;}模拟，更多参考:《你不知道的javascript》上卷中3.4.2节 使用let，const是最佳实践 需要区分函数声明和函数表达式 尽量不要写有提升的代码（声明尽量提前）","categories":[],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://skyitachi.github.io/tags/javascript/"}]}],"categories":[],"tags":[{"name":"algorithm hnsw 向量数据库 Lucene Indexing","slug":"algorithm-hnsw-向量数据库-Lucene-Indexing","permalink":"https://skyitachi.github.io/tags/algorithm-hnsw-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-Lucene-Indexing/"},{"name":"ES, Lucene, elasticsearch","slug":"ES-Lucene-elasticsearch","permalink":"https://skyitachi.github.io/tags/ES-Lucene-elasticsearch/"},{"name":"algorithm 向量数据库 vector ann","slug":"algorithm-向量数据库-vector-ann","permalink":"https://skyitachi.github.io/tags/algorithm-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-vector-ann/"},{"name":"java, Exception, CompletableFuture, ThreadPool, batch","slug":"java-Exception-CompletableFuture-ThreadPool-batch","permalink":"https://skyitachi.github.io/tags/java-Exception-CompletableFuture-ThreadPool-batch/"},{"name":"algorithm hnsw 向量数据库 faiss hnswlib","slug":"algorithm-hnsw-向量数据库-faiss-hnswlib","permalink":"https://skyitachi.github.io/tags/algorithm-hnsw-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93-faiss-hnswlib/"},{"name":"algorithm hnsw 向量数据库","slug":"algorithm-hnsw-向量数据库","permalink":"https://skyitachi.github.io/tags/algorithm-hnsw-%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"golang slice","slug":"golang-slice","permalink":"https://skyitachi.github.io/tags/golang-slice/"},{"name":"algorithm","slug":"algorithm","permalink":"https://skyitachi.github.io/tags/algorithm/"},{"name":"dubbo, go, protobuf","slug":"dubbo-go-protobuf","permalink":"https://skyitachi.github.io/tags/dubbo-go-protobuf/"},{"name":"cpp","slug":"cpp","permalink":"https://skyitachi.github.io/tags/cpp/"},{"name":"javascript","slug":"javascript","permalink":"https://skyitachi.github.io/tags/javascript/"}]}